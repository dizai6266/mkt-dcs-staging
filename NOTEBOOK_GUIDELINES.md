# Databricks Notebook å¼€å‘è§„èŒƒ (NOTEBOOK_GUIDELINES.md)

> æœ¬è§„èŒƒé€‚ç”¨äºä» Airflow DAG è¿ç§»åˆ° Databricks Notebook çš„æ•°æ®æŠ¥å‘Šä»»åŠ¡ã€‚éµå¾ªæœ¬è§„èŒƒå¯ç¡®ä¿ä»£ç ä¸€è‡´æ€§ï¼Œä¾¿äºç»´æŠ¤å’Œ AI è¾…åŠ©å¼€å‘ã€‚

---

## ç›®å½•ç»“æ„

```
mkt-dcs-staging/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ helper.py           # æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆä¸Šä¼ ã€ä¿å­˜ã€é€šçŸ¥ç­‰ï¼‰
â”‚   â””â”€â”€ config_manager.py   # é…ç½®ç®¡ç†ï¼ˆç¯å¢ƒã€å¯†é’¥ã€S3è·¯å¾„ï¼‰
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ iap/
â”‚   â”‚   â””â”€â”€ amazon_iap_report.py
â”‚   â”œâ”€â”€ spend/
â”‚   â”‚   â”œâ”€â”€ applovin_asset_spend_report.py
â”‚   â”‚   â””â”€â”€ apple_search_spend_report.py
â”‚   â””â”€â”€ income/
â”‚       â””â”€â”€ ...
â””â”€â”€ data_output/            # æœ¬åœ°è¾“å‡ºç›®å½•ï¼ˆstaging/dev æ¨¡å¼ï¼‰
```

---

## Notebook æ ‡å‡†ç»“æ„ï¼ˆ6 ä¸ªéƒ¨åˆ†ï¼‰

æ¯ä¸ª Notebook å¿…é¡»åŒ…å«ä»¥ä¸‹ 6 ä¸ªéƒ¨åˆ†ï¼ŒæŒ‰é¡ºåºæ’åˆ—ï¼š

### Part 1: æ ‡é¢˜ä¸è¯´æ˜

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # {å¹¿å‘Šç½‘ç»œ} {æŠ¥å‘Šç±»å‹} Report
# MAGIC
# MAGIC ç®€è¦è¯´æ˜è¯¥ Notebook çš„åŠŸèƒ½ã€‚
```

**ç¤ºä¾‹** [1]ï¼š
```python
# MAGIC # Amazon IAP Report
# MAGIC
# MAGIC è¯¥ Notebook ä» Amazon API è·å– IAP é”€å”®æŠ¥å‘Šæ•°æ®ã€‚
```

---

### Part 2: Setup & Imports

```python
# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Setup & Imports

# COMMAND ----------

import gzip
import io
import json
import os
import shutil
import zipfile
from datetime import datetime, timedelta
import sys

import pandas as pd
import requests

# åŠ¨æ€æ·»åŠ å½“å‰ç›®å½•åˆ° sys.path
current_dir = os.getcwd()
if current_dir not in sys.path:
    sys.path.append(current_dir)

from utils import helper
from utils.config_manager import get_env_mode, setup_feishu_notify
import importlib
importlib.reload(helper)

# è®¾ç½®é£ä¹¦é€šçŸ¥
Notifier = setup_feishu_notify()

print(f"ğŸ”§ Environment Mode: {get_env_mode()}")
print(f"âœ… Environment Setup Complete. Current Dir: {os.getcwd()}")
```

---

### Part 3: Configuration

```python
# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configuration

# COMMAND ----------

# --- [é…ç½®å‚æ•°] ---
_AD_NETWORK = '{å¹¿å‘Šç½‘ç»œå}'    # ä¾‹å¦‚: 'amazon', 'applovin', 'apple_search'
_AD_TYPE = '{æŠ¥å‘Šç±»å‹}'          # å¯é€‰å€¼: 'iap', 'spend', 'income', 'attribution'

# --- [æ—¥æœŸå‚æ•°] ---
try:
    dbutils.widgets.text("ds", "", "Date (YYYY-MM-DD)")
    ds_param = dbutils.widgets.get("ds")
except:
    ds_param = ""

if not ds_param:
    ds_param = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

print(f"ğŸ“… Execution Date: {ds_param}")
```

**é…ç½®å‚æ•°è¯´æ˜**ï¼š

| å˜é‡ | è¯´æ˜ | ç¤ºä¾‹å€¼ |
|------|------|--------|
| `_AD_NETWORK` | å¹¿å‘Šç½‘ç»œæ ‡è¯†ï¼ˆå°å†™ï¼‰ | `'amazon'`, `'applovin'`, `'apple_search'` |
| `_AD_TYPE` | æŠ¥å‘Šç±»å‹ | `'iap'`, `'spend'`, `'income'`, `'attribution'` |
| `ds_param` | æ‰§è¡Œæ—¥æœŸ | `'2025-12-09'` |

---

### Part 4: Core Functions

```python
# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Core Functions

# COMMAND ----------

# åœ¨æ­¤å®šä¹‰æ‰€æœ‰ä¸šåŠ¡é€»è¾‘å‡½æ•°
```

#### 4.1 æ ¸å¿ƒå‡½æ•°å‘½åè§„èŒƒ

| å‡½æ•°ç±»å‹ | å‘½åæ ¼å¼ | ç¤ºä¾‹ |
|----------|----------|------|
| ä¸»ä»»åŠ¡å‡½æ•° | `fetch_{type}_report_task(ds)` | `fetch_iap_report_task(ds)` |
| æ•°æ®å¤„ç†å‡½æ•° | `_process_and_upload(...)` | `_process_and_upload(file_path, year, month, ds, client_index)` |
| API è°ƒç”¨å‡½æ•° | `_get_{resource}(...)` | `_get_access_token(...)`, `_get_sale_report_url(...)` |
| è¾…åŠ©å‡½æ•° | `_helper_name(...)` | `_get_month_last_day(year, month)` |

#### 4.2 ä¸»ä»»åŠ¡å‡½æ•°æ¨¡æ¿

```python
def fetch_{type}_report_task(ds: str):
    """
    è·å– {AD_NETWORK} {TYPE} æŠ¥å‘Š
    
    Args:
        ds: æ‰§è¡Œæ—¥æœŸ (YYYY-MM-DD)
    """
    print(f"ğŸ“Š Fetching {_AD_NETWORK} report for {ds}")
    
    # 1. è·å–é…ç½®
    cfg = helper.get_cfg('{config_name}')
    
    # 2. éå†è´¦å·/å®¢æˆ·ç«¯
    for index, item in enumerate(cfg.get('{key}'), start=1):
        print(f"\n   ğŸ“± Processing item {index}...")
        
        # 3. è·å–æ•°æ®
        # ...
        
        # 4. å¤„ç†å¹¶ä¿å­˜
        helper.save_report(
            ad_network=_AD_NETWORK,
            ad_type=_AD_TYPE,
            report=report_data,      # æ”¯æŒ CSV/JSON/JSONL æ ¼å¼ï¼Œè‡ªåŠ¨è½¬æ¢
            exc_ds=ds,
            start_ds=start_date,
            end_ds=end_date,
            custom=index             # å¯é€‰ï¼šç”¨äºåŒºåˆ†å¤šè´¦å·
        )
        
        print(f"   âœ… Processed item {index}")
    
    print(f"\nâœ… Saved {_AD_NETWORK} report for {ds}")
```

#### 4.3 helper.save_report() å‚æ•°è¯´æ˜

```python
helper.save_report(
    ad_network: str,      # å¿…å¡«ï¼šå¹¿å‘Šç½‘ç»œå
    ad_type: str,         # å¿…å¡«ï¼šæŠ¥å‘Šç±»å‹
    report: str,          # å¿…å¡«ï¼šæŠ¥å‘Šæ•°æ®ï¼ˆæ”¯æŒ CSV/JSON/JSONLï¼Œè‡ªåŠ¨æ£€æµ‹è½¬æ¢ï¼‰
    exc_ds: str,          # å¿…å¡«ï¼šæ‰§è¡Œæ—¥æœŸ
    start_ds: str,        # å¯é€‰ï¼šæ•°æ®å¼€å§‹æ—¥æœŸ
    end_ds: str,          # å¯é€‰ï¼šæ•°æ®ç»“æŸæ—¥æœŸ
    report_ds: str,       # å¯é€‰ï¼šæŠ¥å‘Šæ—¥æœŸï¼ˆä¸ start_ds/end_ds äºŒé€‰ä¸€ï¼‰
    custom: any,          # å¯é€‰ï¼šè‡ªå®šä¹‰æ ‡è¯†ï¼ˆç”¨äºæ–‡ä»¶ååŒºåˆ†å¤šè´¦å·ï¼‰
    data_format: str      # å¯é€‰ï¼šå¼ºåˆ¶æŒ‡å®šæ ¼å¼ ('csv'/'jsonl'/'json_array')
)
```

**ç”Ÿæˆçš„æ–‡ä»¶åè§„åˆ™**ï¼š

| å‚æ•°ç»„åˆ | æ–‡ä»¶åæ ¼å¼ | ç¤ºä¾‹ |
|----------|------------|------|
| `custom` + `start_ds` + `end_ds` | `{network}_{custom}_{start}_to_{end}` | `applovin_1_2025-12-02_to_2025-12-08` |
| `start_ds` + `end_ds` | `{network}_{start}_to_{end}` | `amazon_2025-12-01_to_2025-12-31` |
| `report_ds` | `{network}_{report_ds}` | `facebook_2025-12-09` |

**æ”¯æŒçš„æ•°æ®æ ¼å¼**ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰ï¼š

| æ ¼å¼ | è¯†åˆ«ç‰¹å¾ | å¤„ç†æ–¹å¼ |
|------|----------|----------|
| JSONL | æ¯è¡Œä»¥ `{` å¼€å¤´ `}` ç»“å°¾ | ç›´æ¥éªŒè¯ï¼Œä¸è½¬æ¢ |
| JSON Array | ä»¥ `[` å¼€å¤´ | è½¬æ¢ä¸º JSONL |
| JSON Object | ä»¥ `{` å¼€å¤´ï¼ˆå•è¡Œï¼‰ | è½¬æ¢ä¸ºå•è¡Œ JSONL |
| **API Response** ğŸ†• | `{"code":200,"results":[...]}` | è‡ªåŠ¨æå– `results` æ•°ç»„ |
| CSV | å…¶ä»–æƒ…å†µ | è½¬æ¢ä¸º JSONL |

**ç‰¹åˆ«è¯´æ˜**ï¼š
- `API Response` æ ¼å¼ä¼šè‡ªåŠ¨è¯†åˆ« `results`, `data`, `items`, `records` ç­‰å¸¸è§å­—æ®µ
- å³ä½¿æ˜¯å¤§æ–‡ä»¶ï¼ˆæµå¼å¤„ç†åªè¯»å– 4KBï¼‰ï¼Œä¹Ÿèƒ½é€šè¿‡å¯å‘å¼æ–¹æ³•æ£€æµ‹æ ¼å¼

---

### Part 5: Execution

```python
# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Execution

# COMMAND ----------

print(f"ğŸš€ Starting Job for {_AD_NETWORK}")

try:
    fetch_{type}_report_task(ds_param)
    print("\nâœ… Job Finished Successfully")

except Exception as e:
    print(f"\nâŒ Job Failed: {e}")
    helper.failure_callback(str(e), f"{_AD_NETWORK}_{_AD_TYPE}_report")
    raise e  # å¿…é¡»é‡æ–°æŠ›å‡ºï¼Œä¿æŒ Job å¤±è´¥çŠ¶æ€
```

---

### Part 6: Data Validation

```python
# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Data Validation

# COMMAND ----------

env_mode = get_env_mode()
print(f"\nğŸ” Data Validation (ENV_MODE={env_mode})")

if env_mode != 'staging':
    print("âš ï¸ é staging æ¨¡å¼ï¼Œè·³è¿‡æœ¬åœ° previewã€‚")
else:
    try:
        base_root = getattr(helper, "_DATA_BASE_PATH", None) or os.path.join(os.getcwd(), "data_output")
        preview_root = os.path.join(base_root, _AD_TYPE, _AD_NETWORK)
        print(f"ğŸ” Scanning preview files under: {preview_root}")
        
        if not os.path.exists(preview_root):
            print(f"âš ï¸ Preview directory does not exist: {preview_root}")
        else:
            preview_files = []
            for root, dirs, files in os.walk(preview_root):
                for name in files:
                    if name.endswith('.preview'):
                        preview_files.append(os.path.join(root, name))
            
            print(f"âœ… Found {len(preview_files)} preview file(s)")
            
            for sample_file in preview_files:
                print(f"\n   Previewing: {sample_file}")
                try:
                    df = pd.read_json(sample_file, lines=True)
                    try:
                        display(df.head(5))
                    except NameError:
                        print(df.head(5).to_string())
                    print(f"   Total rows: {len(df)}\n")
                except Exception as e:
                    print(f"   âŒ Failed to read preview file: {e}")
    except Exception as e:
        print(f"âŒ Preview scan error: {e}")
```

---

## ç¯å¢ƒæ¨¡å¼è¯´æ˜

| æ¨¡å¼ | æœ¬åœ°æ–‡ä»¶ | S3 ä¸Šä¼  | ç”¨é€” |
|------|----------|---------|------|
| `dev` | å®Œæ•´æ•°æ® (`.jsonl`) | âŒ | æœ¬åœ°å¼€å‘è°ƒè¯• |
| `staging` | 5MB é¢„è§ˆ (`.preview`) | âœ… (`reports_staging/`) | æµ‹è¯•éªŒè¯ |
| `prod` | âŒ | âœ… (`reports/`) | ç”Ÿäº§ç¯å¢ƒ |

---

## å¸¸è§è¿ç§»æ¨¡å¼

### æ¨¡å¼ Aï¼šå•è´¦å· + å•æ—¥æœŸèŒƒå›´ï¼ˆæœ€ç®€å•ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šAarki, AppLovin Asset ç­‰
**å‚è€ƒæ¨¡æ¿**ï¼š`aarki_spend_report.py` â­

```python
def fetch_spend_report_task(ds: str):
    cfg = helper.get_cfg(_AD_NETWORK)
    
    req_opt = dict(
        url='https://api.example.com/report',
        params={'token': cfg.get('token'), 'start': start_ds, 'end': end_ds}
    )
    
    # ä¸€è¡Œæå®šï¼è‡ªåŠ¨å¤„ç†æ ¼å¼æ£€æµ‹ã€è½¬æ¢ã€ä¸Šä¼ 
    helper.fetch_report(
        ad_network=_AD_NETWORK,
        ad_type=_AD_TYPE,
        exc_ds=ds,
        start_ds=start_ds,
        end_ds=end_ds,
        **req_opt
    )
```

### æ¨¡å¼ Bï¼šå¤šè´¦å·

**é€‚ç”¨åœºæ™¯**ï¼šAppLovinï¼ˆå¤šä¸ª api_keyï¼‰

```python
def fetch_spend_report_task(ds: str):
    cfg = helper.get_cfg('applovin')
    
    for item in cfg.get('spend'):
        account_index = item.get('index')
        api_key = item.get('api_key')
        
        req_opt = dict(url='...', params={'api_key': api_key, ...})
        
        helper.fetch_report(
            ad_network=_AD_NETWORK,
            ad_type=_AD_TYPE,
            exc_ds=ds,
            start_ds=start_ds,
            end_ds=end_ds,
            custom=account_index,  # åŒºåˆ†å¤šè´¦å·æ–‡ä»¶å
            **req_opt
        )
```

### æ¨¡å¼ Cï¼šAPI å“åº”åŒ…è£…æ ¼å¼

**é€‚ç”¨åœºæ™¯**ï¼šAppLovin Income/MAX Revenueï¼ˆè¿”å› `{"code":200,"results":[...]}`ï¼‰
**å‚è€ƒæ¨¡æ¿**ï¼š`applovin_income_report.py`

```python
# æ— éœ€ç‰¹æ®Šå¤„ç†ï¼ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«å¹¶æå– results æ•°ç»„
helper.fetch_report(
    ad_network=_AD_NETWORK,
    ad_type=_AD_TYPE,
    exc_ds=ds,
    start_ds=start_ds,
    end_ds=end_ds,
    **req_opt
)
```

### æ¨¡å¼ Dï¼šéœ€è¦å±•å¼€åµŒå¥—å­—æ®µ ğŸ†•

**é€‚ç”¨åœºæ™¯**ï¼šAppLovin MAX Configï¼ˆéœ€è¦å±•å¼€ `ad_network_settings`ï¼‰
**å‚è€ƒæ¨¡æ¿**ï¼š`applovin_max_report.py`

```python
from utils.data_parser import convert_applovin_max_config

def fetch_max_report_task(ds: str):
    all_records = []
    
    for ad_unit in ad_units:
        response = requests.get(f'https://api.../ad_unit/{ad_unit}', ...)
        
        # ä½¿ç”¨ä¸“ç”¨è½¬æ¢å™¨å±•å¼€ ad_network_settings
        jsonl_content, row_count = convert_applovin_max_config(response.text)
        
        for line in jsonl_content.split('\n'):
            if line.strip():
                all_records.append(line)
    
    # ä¿å­˜åˆå¹¶åçš„æ•°æ®
    helper.save_report(
        ad_network=_AD_NETWORK,
        ad_type=_AD_TYPE,
        report_content='\n'.join(all_records),
        exc_ds=ds,
        start_ds=start_ds,
        end_ds=end_ds,
        data_format='jsonl'
    )
```

**å±•å¼€æ•ˆæœ**ï¼š
```
è¾“å…¥: {"id":"xxx", "ad_network_settings": {"UNITY": {...}, "ADMOB": {...}}}
è¾“å‡º:
{"id":"xxx", "network":"UNITY", "ad_network_ad_unit_id":"..."}
{"id":"xxx", "network":"ADMOB", "ad_network_ad_unit_id":"..."}
```

### æ¨¡å¼ Eï¼šå¤šå±‚åµŒå¥— + åˆå¹¶

**é€‚ç”¨åœºæ™¯**ï¼šApple Search Adsï¼ˆOrg â†’ Campaign â†’ Keywordsï¼‰

```python
def fetch_spend_report_task(ds: str):
    all_data = []
    
    for org in cfg.get('spend'):
        campaigns = _get_campaigns(org)
        
        for campaign in campaigns:
            report = _get_campaign_report(campaign)
            detail_data = _parse_detail_data(report, campaign_info=campaign)
            all_data.extend(detail_data)
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®åä¿å­˜
    helper.save_report(
        ad_network=_AD_NETWORK,
        ad_type=_AD_TYPE,
        report=json.dumps(all_data),
        exc_ds=ds,
        report_ds=ds
    )
```

---

## æ•°æ®æ ¼å¼è§£æå™¨ (data_parser.py) ğŸ†•

`utils/data_parser.py` æä¾›äº†å¼ºå¤§çš„æ•°æ®æ ¼å¼è‡ªåŠ¨è¯†åˆ«å’Œè½¬æ¢åŠŸèƒ½ã€‚

### æ”¯æŒçš„æ•°æ®æ ¼å¼

| æ ¼å¼ | ç¤ºä¾‹ | è‡ªåŠ¨å¤„ç† |
|------|------|----------|
| CSV | `col1,col2\nval1,val2` | âœ… è½¬æ¢ä¸º JSONL |
| JSONL | `{"a":1}\n{"a":2}` | âœ… ç›´æ¥éªŒè¯ |
| JSON Array | `[{"a":1},{"a":2}]` | âœ… è½¬æ¢ä¸º JSONL |
| JSON Object | `{"a":1}` | âœ… è½¬æ¢ä¸ºå•è¡Œ JSONL |
| API Response | `{"code":200,"results":[...]}` | âœ… æå– `results` æ•°ç»„ |

### æ ¸å¿ƒå‡½æ•°

```python
from utils.data_parser import (
    detect_format,           # æ£€æµ‹æ•°æ®æ ¼å¼
    convert_to_jsonl,        # è½¬æ¢ä¸º JSONL
    expand_applovin_max_ad_unit,  # å±•å¼€ ad_network_settings
    convert_applovin_max_config,  # AppLovin MAX é…ç½®è½¬æ¢
)
```

### ä½¿ç”¨ç¤ºä¾‹

#### 1. æ ¼å¼æ£€æµ‹

```python
from utils.data_parser import detect_format, DataFormat

# API å“åº”æ ¼å¼
fmt = detect_format('{"code":200,"results":[{"day":"2025-12-09"}]}')
print(fmt)  # DataFormat.API_RESPONSE

# CSV æ ¼å¼
fmt = detect_format('day,revenue\n2025-12-09,100')
print(fmt)  # DataFormat.CSV
```

#### 2. è½¬æ¢ä¸º JSONL

```python
from utils.data_parser import convert_to_jsonl

# API å“åº”è‡ªåŠ¨æå– results
api_resp = '{"code":200,"results":[{"day":"2025-12-09","revenue":100}]}'
jsonl, count, fmt = convert_to_jsonl(api_resp)
print(f"Converted {count} rows")
print(jsonl)  # {"day":"2025-12-09","revenue":100}
```

#### 3. å±•å¼€ AppLovin MAX é…ç½®

```python
from utils.data_parser import expand_applovin_max_ad_unit

ad_unit = {
    "id": "abc123",
    "name": "Test_iOS_Inter",
    "platform": "ios",
    "ad_network_settings": {
        "UNITY_BIDDING": {"disabled": False, "ad_network_ad_unit_id": "unity_xxx"},
        "ADMOB_BIDDING": {"disabled": False, "ad_network_ad_unit_id": "admob_yyy"}
    }
}

records = expand_applovin_max_ad_unit(ad_unit)
# è¾“å‡º 2 æ¡è®°å½•ï¼Œæ¯ä¸ª network ä¸€è¡Œ
for r in records:
    print(r)
# {'id': 'abc123', 'name': 'Test_iOS_Inter', 'platform': 'ios', 'network': 'UNITY_BIDDING', 'ad_network_ad_unit_id': 'unity_xxx'}
# {'id': 'abc123', 'name': 'Test_iOS_Inter', 'platform': 'ios', 'network': 'ADMOB_BIDDING', 'ad_network_ad_unit_id': 'admob_yyy'}
```

### æ·»åŠ è‡ªå®šä¹‰è½¬æ¢å™¨

å¦‚æœé‡åˆ°æ–°çš„åµŒå¥—æ•°æ®æ ¼å¼ï¼Œå¯ä»¥åœ¨ `data_parser.py` ä¸­æ·»åŠ æ–°çš„è½¬æ¢å‡½æ•°ï¼š

```python
def expand_your_nested_data(data: dict) -> List[dict]:
    """
    å±•å¼€ä½ çš„åµŒå¥—æ•°æ®
    
    å‚è€ƒ expand_applovin_max_ad_unit() çš„å®ç°
    """
    base_fields = ['id', 'name', ...]
    base_record = {k: data.get(k) for k in base_fields}
    
    nested_items = data.get('nested_field', [])
    expanded = []
    
    for item in nested_items:
        record = base_record.copy()
        record['nested_key'] = item.get('key')
        expanded.append(record)
    
    return expanded
```

---

## è¾…åŠ©å‡½æ•°åº“

### è·å–æœˆä»½æœ€åä¸€å¤©

```python
def _get_month_last_day(year: int, month: int) -> int:
    """è·å–æŒ‡å®šæœˆä»½çš„æœ€åä¸€å¤©"""
    if month == 12:
        next_month_first = datetime(year + 1, 1, 1)
    else:
        next_month_first = datetime(year, month + 1, 1)
    return (next_month_first - timedelta(days=1)).day
```

### CSV æ·»åŠ é¢å¤–åˆ—

```python
def _add_columns_to_csv(csv_str: str, extra_columns: dict) -> str:
    """ç»™ CSV æ•°æ®æ·»åŠ é¢å¤–åˆ—"""
    lines = csv_str.strip().split('\n')
    if not lines:
        return csv_str
    
    # æ·»åŠ  header
    extra_keys = ','.join(extra_columns.keys())
    header = f"{lines[0]},{extra_keys}"
    
    # æ·»åŠ æ•°æ®
    extra_values = ','.join(str(v) for v in extra_columns.values())
    modified_lines = [header]
    for line in lines[1:]:
        if line.strip():
            modified_lines.append(f"{line},{extra_values}")
    
    return '\n'.join(modified_lines)
```

---

## Checklist

### æ–°å¢ Notebook å‰ï¼Œè¯·ç¡®è®¤ä»¥ä¸‹äº‹é¡¹ï¼š

**åŸºç¡€é…ç½®**
- [ ] è®¾ç½®æ­£ç¡®çš„ `_AD_NETWORK` å’Œ `_AD_TYPE`
- [ ] é…ç½®å·²æ·»åŠ åˆ° Databricks Secretsï¼ˆ`secret_{network_name}`ï¼‰
- [ ] ä¸»å‡½æ•°å‘½åéµå¾ª `fetch_{type}_report_task(ds)` æ ¼å¼

**æ•°æ®å¤„ç†**
- [ ] ä½¿ç”¨ `helper.fetch_report()` æˆ– `helper.save_report()` ä¿å­˜æ•°æ®
- [ ] å¦‚æœæ˜¯ API å“åº”åŒ…è£…æ ¼å¼ï¼ˆ`{"code":200,"results":[...]}`ï¼‰ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†
- [ ] å¦‚æœæœ‰åµŒå¥—å­—æ®µéœ€è¦å±•å¼€ï¼Œæ·»åŠ è‡ªå®šä¹‰è½¬æ¢å™¨ï¼ˆå‚è€ƒ `expand_applovin_max_ad_unit()`ï¼‰

**é”™è¯¯å¤„ç†**
- [ ] åŒ…å« try-except å’Œ `helper.failure_callback()`
- [ ] åŒ…å« Data Validation éƒ¨åˆ†

**æµ‹è¯•éªŒè¯**
- [ ] åœ¨ staging ç¯å¢ƒæµ‹è¯•é€šè¿‡
- [ ] æ£€æŸ¥ `data_output/raw_download/` ä¸‹çš„åŸå§‹å“åº”ï¼ˆè°ƒè¯•ç”¨ï¼‰
- [ ] Preview æ–‡ä»¶å¯æ­£å¸¸è¯»å–ï¼ˆ`pd.read_json(file, lines=True)`ï¼‰
- [ ] è¾“å‡ºæ ¼å¼ç¬¦åˆé¢„æœŸï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰

---

## å¸¸è§é—®é¢˜æ’æŸ¥

| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|----------|----------|
| Preview æ–‡ä»¶è¯»å–å¤±è´¥ | JSON æ ¼å¼é”™è¯¯ | æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Šå­—ç¬¦ï¼Œä½¿ç”¨ `ensure_ascii=False` |
| å¤šè´¦å·æ–‡ä»¶è¦†ç›– | æœªä½¿ç”¨ `custom` å‚æ•° | æ·»åŠ  `custom=index` åŒºåˆ†æ–‡ä»¶å |
| S3 ä¸Šä¼ å¤±è´¥ | é…ç½®ç¼ºå¤± | æ£€æŸ¥ Secrets ä¸­çš„ S3 é…ç½® |
| æ•°æ®è¢«æˆªæ–­ | 5MB preview é™åˆ¶ | æ­£å¸¸ç°è±¡ï¼Œå®Œæ•´æ•°æ®åœ¨ S3 |
| **æ ¼å¼æ£€æµ‹ä¸º UNKNOWN** ğŸ†• | æµå¼å¤„ç†åªè¯»å– 4KB | ç³»ç»Ÿä¼šè‡ªåŠ¨é‡æ–°æ£€æµ‹ï¼Œæˆ–æ£€æŸ¥ `.raw` æ–‡ä»¶ |
| **API å“åº”æœªæ­£ç¡®è§£æ** ğŸ†• | éæ ‡å‡†åŒ…è£…æ ¼å¼ | æ£€æŸ¥å­—æ®µåæ˜¯å¦åœ¨ `API_RESPONSE_DATA_KEYS` ä¸­ |
| **åµŒå¥—æ•°æ®æœªå±•å¼€** ğŸ†• | éœ€è¦è‡ªå®šä¹‰è½¬æ¢å™¨ | å‚è€ƒ `expand_applovin_max_ad_unit()` æ·»åŠ è½¬æ¢å™¨ |
| **Preview æ˜¾ç¤ºåŸå§‹æ ¼å¼** ğŸ†• | æ ¼å¼è½¬æ¢å¤±è´¥ | æ£€æŸ¥ `raw_download/` ä¸‹çš„åŸå§‹å“åº”æ ¼å¼ |

### è°ƒè¯•æ­¥éª¤

1. **æŸ¥çœ‹åŸå§‹å“åº”**ï¼ˆstaging æ¨¡å¼ï¼‰ï¼š
   ```
   data_output/raw_download/{ad_type}/{ad_network}/{date}/*.raw
   ```

2. **æµ‹è¯•æ ¼å¼æ£€æµ‹**ï¼š
   ```python
   from utils.data_parser import detect_format
   with open('xxx.raw', 'r') as f:
       sample = f.read(1000)
   print(detect_format(sample))
   ```

3. **æ‰‹åŠ¨æµ‹è¯•è½¬æ¢**ï¼š
   ```python
   from utils.data_parser import convert_to_jsonl
   jsonl, count, fmt = convert_to_jsonl(sample_data)
   print(f"Format: {fmt}, Rows: {count}")
   ```