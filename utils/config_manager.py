import os
import logging
import json
import base64
import os
os.environ['ENV_MODE'] = 'staging'

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# ===== é›†ä¸­é…ç½®åŒºåŸŸ =====
# åœ¨è¿™é‡Œä¿®æ”¹é»˜è®¤ç¯å¢ƒæ¨¡å¼ï¼š'staging' æˆ– 'prod'
# å¦‚æœè®¾ç½®äº†ç¯å¢ƒå˜é‡ ENV_MODEï¼Œä¼šä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
DEFAULT_ENV_MODE = 'staging'  # ä¿®æ”¹è¿™é‡Œå³å¯æ”¹å˜æ‰€æœ‰æŠ¥å‘Šçš„é»˜è®¤ç¯å¢ƒæ¨¡å¼

# feishu-notify è·¯å¾„é…ç½®
FEISHU_NOTIFY_PATH_PROD = '/Workspace/Repos/Shared/feishu-notify'
FEISHU_NOTIFY_PATH_STAGING = '/Workspace/Users/dizai@joycastle.mobi/feishu-notify'
# ========================

# æ£€æµ‹è¿è¡Œç¯å¢ƒ
IS_DATABRICKS = False
try:
    from pyspark.dbutils import DBUtils
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.getOrCreate()
    dbutils = DBUtils(spark)
    IS_DATABRICKS = True
except ImportError:
    pass

def _find_project_root():
    """
    æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•ï¼ˆé€šè¿‡æŸ¥æ‰¾åŒ…å« config ç›®å½•çš„ä½ç½®ï¼‰
    è¿”å›é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›å½“å‰å·¥ä½œç›®å½•
    """
    search_dir = os.getcwd()
    
    # å‘ä¸ŠæŸ¥æ‰¾ï¼Œæ‰¾åˆ°åŒ…å« config ç›®å½•çš„ä½ç½®
    for _ in range(10):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾10å±‚
        config_dir = os.path.join(search_dir, 'config')
        if os.path.exists(config_dir) and os.path.isdir(config_dir):
            return search_dir
        parent = os.path.dirname(search_dir)
        if parent == search_dir:  # åˆ°è¾¾æ ¹ç›®å½•
            break
        search_dir = parent
    
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
    return os.getcwd()

def init_env_mode():
    """
    åˆå§‹åŒ–ç¯å¢ƒæ¨¡å¼
    å¦‚æœç¯å¢ƒå˜é‡ ENV_MODE æœªè®¾ç½®ï¼Œåˆ™ä½¿ç”¨ DEFAULT_ENV_MODE
    æ­¤å‡½æ•°åœ¨æ¨¡å—åŠ è½½æ—¶è‡ªåŠ¨è°ƒç”¨
    """
    if not os.getenv('ENV_MODE'):
        os.environ['ENV_MODE'] = DEFAULT_ENV_MODE
        logger.info(f"ğŸ”§ ENV_MODE not set, using default: {DEFAULT_ENV_MODE}")
    else:
        existing_env_mode = os.getenv('ENV_MODE')
        logger.info(f"ğŸ”§ ENV_MODE from environment: {existing_env_mode}")

def get_env_mode():
    """
    è·å–ç¯å¢ƒæ¨¡å¼ï¼šprod æˆ– staging
    é»˜è®¤è¿”å› DEFAULT_ENV_MODEï¼ˆå¯é€šè¿‡ä¿®æ”¹æ–‡ä»¶é¡¶éƒ¨ DEFAULT_ENV_MODE å˜é‡æ¥æ”¹å˜ï¼‰
    """
    # ç¡®ä¿ç¯å¢ƒæ¨¡å¼å·²åˆå§‹åŒ–
    if not os.getenv('ENV_MODE'):
        init_env_mode()
    
    env_mode = os.getenv('ENV_MODE', DEFAULT_ENV_MODE).lower()
    if env_mode not in ['prod', 'staging']:
        logger.warning(f"âš ï¸ Invalid ENV_MODE: {env_mode}, defaulting to '{DEFAULT_ENV_MODE}'")
        return DEFAULT_ENV_MODE
    return env_mode

# æ¨¡å—åŠ è½½æ—¶è‡ªåŠ¨åˆå§‹åŒ–ç¯å¢ƒæ¨¡å¼
init_env_mode()

def get_feishu_notify_path():
    """
    è·å– feishu-notify æ¨¡å—çš„è·¯å¾„
    - prod: ä½¿ç”¨å…±äº«è·¯å¾„ FEISHU_NOTIFY_PATH_PROD
    - staging: ä½¿ç”¨å¼€å‘è·¯å¾„ FEISHU_NOTIFY_PATH_STAGING
    """
    env_mode = get_env_mode()
    if env_mode == 'prod':
        return FEISHU_NOTIFY_PATH_PROD
    return FEISHU_NOTIFY_PATH_STAGING

def setup_feishu_notify():
    """
    è®¾ç½® feishu-notify æ¨¡å—è·¯å¾„å¹¶å¯¼å…¥ Notifier
    
    Usage:
        from utils.config_manager import setup_feishu_notify
        Notifier = setup_feishu_notify()
    
    Returns:
        Notifier class from feishu-notify module
    """
    import sys
    feishu_path = get_feishu_notify_path()
    if feishu_path not in sys.path:
        sys.path.append(feishu_path)
    
    try:
        from notifier import Notifier
        return Notifier
    except ImportError as e:
        logger.warning(f"âš ï¸ Failed to import Notifier from {feishu_path}: {e}")
        return None

def get_secret_config(config_name):
    """
    ç»Ÿä¸€çš„é…ç½®è·å–å…¥å£
    ä¼˜å…ˆçº§ï¼ˆæŒ‰é¡ºåºå°è¯•ï¼Œå…¨éƒ¨å¤±è´¥æ‰æŠ¥é”™ï¼‰:
    1. Databricks Secretsï¼ˆå¦‚æœå¯ç”¨ï¼‰
    2. ç¯å¢ƒå˜é‡ (SECRET_NAME)
    3. æœ¬åœ°æ–‡ä»¶ variables.json (fallback)
    """
    full_key = f'secret_{config_name}'
    errors = []
    
    # --- ä¼˜å…ˆçº§ 1: Databricks Secrets ---
    if IS_DATABRICKS:
        try:
            secret_val = dbutils.secrets.get(scope="airflow_secrets", key=full_key)
            if secret_val:
                logger.info(f"âœ… Loading config {config_name} from Databricks Secrets: {full_key}")
                try:
                    # å°è¯•ç›´æ¥è§£æ JSON
                    return json.loads(secret_val)
                except json.JSONDecodeError:
                    # å…¼å®¹æ—§çš„ Base64 ç¼–ç æ ¼å¼
                    try:
                        return json.loads(base64.b64decode(secret_val.encode('utf-8')).decode('utf-8'))
                    except Exception as e:
                        logger.warning(f"Failed to decode Databricks secret {full_key}: {e}")
                        errors.append(f"Databricks secret decode error: {e}")
        except Exception as e:
            # Databricks secrets.get å¦‚æœ key ä¸å­˜åœ¨ä¼šæŠ›é”™ï¼Œè¿™æ˜¯æ­£å¸¸çš„
            logger.debug(f"Databricks secret {full_key} not found: {e}")
            errors.append(f"Databricks secret not found: {e}")

    # --- ä¼˜å…ˆçº§ 2: ç¯å¢ƒå˜é‡ ---
    env_key = full_key.upper()
    env_val = os.getenv(env_key)
    
    if env_val:
        logger.info(f"âœ… Loading config {config_name} from Environment Variable: {env_key}")
        try:
            # å°è¯•è§£æ JSON
            return json.loads(env_val)
        except json.JSONDecodeError:
            # å°è¯• Base64 è§£ç  (å…¼å®¹æ—§çš„ Base64 ç¼–ç å­—ç¬¦ä¸²)
            try:
                return json.loads(base64.b64decode(env_val.encode('utf-8')).decode('utf-8'))
            except Exception as e:
                logger.warning(f"Failed to decode env var {env_key}: {e}")
                errors.append(f"Env var decode error: {e}")
    else:
        errors.append(f"Env var {env_key} not set")

    # --- ä¼˜å…ˆçº§ 3: æœ¬åœ°æ–‡ä»¶ variables.json (fallback) ---
    try:
        # æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•
        project_root = _find_project_root()
        
        # åªæŸ¥æ‰¾ config/variables.jsonï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        json_path = os.path.join(project_root, 'config', 'variables.json')
        
        if os.path.exists(json_path):
            try:
                with open(json_path, 'r') as f:
                    all_vars = json.load(f)
                
                if full_key in all_vars:
                    logger.info(f"âœ… Loading config {config_name} from local file: {json_path}")
                    val = all_vars[full_key]
                    
                    if isinstance(val, str):
                        try:
                            return json.loads(val)
                        except json.JSONDecodeError:
                            try:
                                return json.loads(base64.b64decode(val.encode('utf-8')).decode('utf-8'))
                            except:
                                return val
                    return val
            except Exception as e:
                logger.debug(f"Error reading {json_path}: {e}")
                errors.append(f"Local file read error: {e}")
        else:
            logger.debug(f"Config file not found: {json_path}")
            errors.append(f"Local file not found: {json_path}")

    except Exception as e:
        logger.warning(f"Error reading local variables.json: {e}")
        errors.append(f"Local file read error: {e}")

    # --- æ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ ---
    error_msg = f'Miss the config of {config_name}. Tried: {"; ".join(errors)}'
    logger.error(error_msg)
    raise ValueError(error_msg)

def get_s3_config():
    """
    æ ¹æ®ç¯å¢ƒæ¨¡å¼è·å– S3 é…ç½®
    - prod: è¿”å› aws_s3_prod é…ç½®
    - staging: è¿”å› aws_s3_staging é…ç½®
    """
    env_mode = get_env_mode()
    
    if env_mode == 'prod':
        logger.info("ğŸš€ [PROD MODE] Loading prod S3 config")
        return get_secret_config('aws_s3_prod')
    else:
        logger.info("ğŸ§ª [STAGING MODE] Loading staging S3 config")
        return get_secret_config('aws_s3_staging')

def get_dag_s3_path_config():
    """
    è¯»å– dag_id_to_s3_paths.json é…ç½®æ–‡ä»¶
    è¿”å›å­—å…¸ï¼š{dag_id: s3_path_template}
    """
    try:
        # æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•
        project_root = _find_project_root()
        
        # è¯»å– config/dag_id_to_s3_paths.json
        config_path = os.path.join(project_root, 'config', 'dag_id_to_s3_paths.json')
        
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                return json.load(f)
        else:
            logger.warning(f"âš ï¸ dag_id_to_s3_paths.json not found at: {config_path}")
            return {}
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to load dag_id_to_s3_paths.json: {e}")
        return {}

def get_s3_path_for_dag(dag_id: str, exc_ds: str = None):
    """
    æ ¹æ® DAG ID å’Œç¯å¢ƒæ¨¡å¼è·å– S3 è·¯å¾„æ¨¡æ¿ï¼Œå¹¶æ›¿æ¢å˜é‡
    
    æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼š
    1. ç®€å•é…ç½®ï¼š{"dag_id": "reports/spend/xxx/{{ds}}/*"} - æ‰€æœ‰ç¯å¢ƒå…±ç”¨
    2. ç¯å¢ƒåŒºåˆ†ï¼š{"dag_id": {"prod": "reports/spend/xxx/{{ds}}/*", "staging": "staging/reports/spend/xxx/{{ds}}/*"}}
    
    Args:
        dag_id: DAG ID æˆ– job name
        exc_ds: æ‰§è¡Œæ—¥æœŸ (YYYY-MM-DD)ï¼Œç”¨äºæ›¿æ¢ {{ds}}
    
    Returns:
        S3 è·¯å¾„å­—ç¬¦ä¸²ï¼Œå¦‚æœæ‰¾ä¸åˆ°é…ç½®åˆ™è¿”å› None
    """
    path_config = get_dag_s3_path_config()
    env_mode = get_env_mode()
    
    if dag_id not in path_config:
        logger.warning(f"âš ï¸ DAG ID '{dag_id}' not found in dag_id_to_s3_paths.json")
        return None
    
    path_template_config = path_config[dag_id]
    
    # åˆ¤æ–­é…ç½®æ ¼å¼ï¼šå¦‚æœæ˜¯å­—å…¸ï¼Œè¯´æ˜åŒºåˆ†äº†ç¯å¢ƒï¼›å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ‰€æœ‰ç¯å¢ƒå…±ç”¨
    path_template = None
    if isinstance(path_template_config, dict):
        # ç¯å¢ƒåŒºåˆ†é…ç½®
        if env_mode in path_template_config:
            path_template = path_template_config[env_mode]
        elif 'default' in path_template_config:
            # å¦‚æœæ²¡æœ‰å¯¹åº”ç¯å¢ƒçš„é…ç½®ï¼Œä½¿ç”¨ default
            path_template = path_template_config['default']
            logger.warning(f"âš ï¸ No {env_mode} path config for {dag_id}, using default")
        else:
            logger.warning(f"âš ï¸ No {env_mode} or default path config for {dag_id}")
            return None
    else:
        # ç®€å•é…ç½®ï¼šæ‰€æœ‰ç¯å¢ƒå…±ç”¨
        path_template = path_template_config
    
    # ç¡®ä¿ path_template æ˜¯å­—ç¬¦ä¸²ç±»å‹
    if not isinstance(path_template, str):
        logger.error(f"âŒ Invalid path_template type for {dag_id}: {type(path_template)}, expected str")
        logger.error(f"   path_template_config: {path_template_config}")
        logger.error(f"   env_mode: {env_mode}")
        logger.error(f"   path_template value: {path_template}")
        if isinstance(path_template, dict):
            logger.error(f"   path_template keys: {list(path_template.keys())}")
        return None
    
    # æ›¿æ¢æ¨¡æ¿å˜é‡
    if exc_ds:
        path_template = path_template.replace('{{ds}}', exc_ds)
    
    # ç§»é™¤æœ«å°¾çš„ /* é€šé…ç¬¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬è¦ä¸Šä¼ å…·ä½“æ–‡ä»¶
    if path_template.endswith('/*'):
        path_template = path_template[:-2]
    
    return path_template


def _mask_sensitive_info(value, max_show=4):
    """éšè—æ•æ„Ÿä¿¡æ¯ï¼Œåªæ˜¾ç¤ºå‰å‡ ä¸ªå­—ç¬¦"""
    if not value:
        return value
    str_value = str(value)
    if len(str_value) <= max_show:
        return '*' * len(str_value)
    return str_value[:max_show] + '*' * (len(str_value) - max_show)


def main():
    """
    æµ‹è¯• main å‡½æ•°ï¼šæ‰“å°é…ç½®ç®¡ç†å™¨çš„ç›¸å…³ä¿¡æ¯
    ç”¨äºè°ƒè¯•å’ŒéªŒè¯é…ç½®åŠ è½½æƒ…å†µ
    """
    print("=" * 80)
    print("ğŸ“‹ Config Manager Test Report")
    print("=" * 80)
    
    # 1. ç¯å¢ƒä¿¡æ¯
    print("\nğŸ”§ Environment Information:")
    print(f"  - IS_DATABRICKS: {IS_DATABRICKS}")
    print(f"  - DEFAULT_ENV_MODE: {DEFAULT_ENV_MODE}")
    
    # æ˜¾ç¤ºç¯å¢ƒå˜é‡çš„åŸå§‹å€¼ï¼ˆåœ¨ init_env_mode ä¹‹å‰ï¼‰
    env_mode_before_init = os.getenv('ENV_MODE')
    print(f"  - ENV_MODE (env var, before init): {env_mode_before_init if env_mode_before_init else 'Not set'}")
    
    env_mode = get_env_mode()
    print(f"  - Current ENV_MODE (resolved): {env_mode}")
    
    # 2. é¡¹ç›®æ ¹ç›®å½•
    print("\nğŸ“ Project Root:")
    project_root = _find_project_root()
    print(f"  - Project Root: {project_root}")
    print(f"  - Current Working Directory: {os.getcwd()}")
    
    # 3. é…ç½®æ–‡ä»¶è·¯å¾„
    print("\nğŸ“„ Configuration Files:")
    variables_path = os.path.join(project_root, 'config', 'variables.json')
    dag_paths_path = os.path.join(project_root, 'config', 'dag_id_to_s3_paths.json')
    print(f"  - variables.json: {variables_path}")
    print(f"    Exists: {os.path.exists(variables_path)}")
    print(f"  - dag_id_to_s3_paths.json: {dag_paths_path}")
    print(f"    Exists: {os.path.exists(dag_paths_path)}")
    
    # 4. S3 é…ç½®ï¼ˆéšè—æ•æ„Ÿä¿¡æ¯ï¼‰
    print("\nâ˜ï¸  S3 Configuration:")
    try:
        s3_config = get_s3_config()
        print(f"  - Bucket: {s3_config.get('bucket', 'N/A')}")
        print(f"  - AWS Key: {_mask_sensitive_info(s3_config.get('aws_key', 'N/A'))}")
        print(f"  - AWS Secret: {_mask_sensitive_info(s3_config.get('aws_secret', 'N/A'))}")
        print(f"  - Config Keys: {list(s3_config.keys())}")
    except Exception as e:
        print(f"  âŒ Failed to load S3 config: {e}")
    
    # 5. DAG S3 è·¯å¾„é…ç½®
    print("\nğŸ—ºï¸  DAG S3 Path Configuration:")
    try:
        dag_path_config = get_dag_s3_path_config()
        print(f"  - Total DAGs configured: {len(dag_path_config)}")
        if dag_path_config:
            print(f"  - DAG IDs: {list(dag_path_config.keys())[:10]}")  # åªæ˜¾ç¤ºå‰10ä¸ª
            if len(dag_path_config) > 10:
                print(f"    ... and {len(dag_path_config) - 10} more")
            
            # æµ‹è¯•å‡ ä¸ªç¤ºä¾‹ DAG çš„è·¯å¾„è§£æ
            print("\n  ğŸ“ Example DAG Path Resolution:")
            test_dag_ids = list(dag_path_config.keys())[:3]  # æµ‹è¯•å‰3ä¸ª
            test_ds = "2024-01-15"
            for dag_id in test_dag_ids:
                s3_path = get_s3_path_for_dag(dag_id, exc_ds=test_ds)
                config_value = dag_path_config[dag_id]
                config_type = "dict (env-specific)" if isinstance(config_value, dict) else "string (shared)"
                print(f"    - {dag_id}:")
                print(f"        Config Type: {config_type}")
                if isinstance(config_value, dict):
                    print(f"        Available Envs: {list(config_value.keys())}")
                print(f"        Resolved Path ({env_mode}): {s3_path}")
    except Exception as e:
        print(f"  âŒ Failed to load DAG path config: {e}")
    
    # 6. æµ‹è¯•é…ç½®è·å–ï¼ˆç¤ºä¾‹ï¼‰
    print("\nğŸ”‘ Configuration Loading Test:")
    test_configs = ['env', 'aws_s3_prod', 'aws_s3_staging']
    for config_name in test_configs:
        try:
            config = get_secret_config(config_name)
            if isinstance(config, dict):
                print(f"  âœ… {config_name}: Loaded (keys: {list(config.keys())})")
                # å¦‚æœæ˜¯ env é…ç½®ï¼Œæ˜¾ç¤ºéƒ¨åˆ†ä¿¡æ¯
                if config_name == 'env' and 'feishu_botid' in config:
                    print(f"      feishu_botid: {_mask_sensitive_info(config.get('feishu_botid'))}")
            else:
                print(f"  âœ… {config_name}: Loaded (type: {type(config).__name__})")
        except Exception as e:
            print(f"  âŒ {config_name}: Failed - {str(e)[:100]}")
    
    # 7. ç¯å¢ƒæ¨¡å¼æµ‹è¯•
    print("\nğŸ§ª Environment Mode Test:")
    print(f"  - Current Mode: {env_mode}")
    print(f"  - S3 Config Source: ", end="")
    if env_mode == 'prod':
        print("aws_s3_prod")
    else:
        print("aws_s3_staging")
    
    print("\n" + "=" * 80)
    print("âœ… Test Complete")
    print("=" * 80)


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—çº§åˆ«ï¼Œè®©æµ‹è¯•è¾“å‡ºæ›´æ¸…æ™°
    logging.basicConfig(
        level=logging.INFO,
        format='%(levelname)s: %(message)s'
    )
    main()
