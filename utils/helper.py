import gzip
import io
import json
import os
import logging
import requests
import pandas
import boto3
from .config_manager import get_s3_config, get_secret_config, get_env_mode, build_s3_path
from .data_parser import (
    DataFormat,
    detect_format,
    convert_to_jsonl,
    StreamingParser,
    records_to_jsonl,
)

_AD_TYPE_INCOME = "income"
_AD_TYPE_SPEND = "spend"
_AD_TYPE_IAP = "iap"
_AD_TYPE_SPEND_MONITOR = "spend_monitor"
_AD_TYPE_ATTRIBUTE = "attribution"

_DATA_BASE_PATH = None
_env_mode = get_env_mode()
_DATA_BASE_PATH = os.path.join(os.getcwd(), "data_output")


# ============================================================================
# é…ç½®ç›¸å…³å‡½æ•°
# ============================================================================

def get_cfg(cfg_name: str):
    """è·å–é…ç½®"""
    if cfg_name == 'env':
        return get_secret_config('env')
    return get_secret_config(cfg_name)


# ============================================================================
# æ–‡ä»¶ä¿å­˜ç›¸å…³å‡½æ•°ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
# ============================================================================

def _save_preview_by_lines(jsonl_content: str, preview_path: str, max_size: int = 5 * 1024 * 1024):
    """
    æŒ‰è¡Œæˆªæ–­ä¿å­˜ previewï¼Œç¡®ä¿ä¸ä¼šæˆªæ–­åˆ° JSON ä¸­é—´
    """
    lines = jsonl_content.split('\n')
    preview_lines = []
    preview_size = 0
    
    for line in lines:
        line_bytes = len((line + '\n').encode('utf-8'))
        if preview_size + line_bytes > max_size:
            break
        preview_lines.append(line)
        preview_size += line_bytes
    
    with open(preview_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(preview_lines))
    
    return preview_size, len(preview_lines)


# ============================================================================
# S3 ä¸Šä¼ ç›¸å…³å‡½æ•°
# ============================================================================

def upload_data_to_s3(data: bytes, s3_subpath: str, exc_ds: str = None, filename: str = None):
    """ç›´æ¥ä»å†…å­˜æ•°æ®ä¸Šä¼ åˆ° S3ï¼ˆå‹ç¼©ä¸º Gzipï¼‰"""
    if not data:
        logging.warning("âš ï¸ No data to upload")
        return
    
    env_mode = get_env_mode()
    
    if env_mode == 'dev':
        logging.info(f"ğŸ”§ [DEV MODE] Skip uploading data to S3")
        return
    
    s3_path_template = build_s3_path(s3_subpath, exc_ds)
    
    if not filename:
        filename = f"{s3_subpath.replace('/', '_')}_{exc_ds}.jsonl"
    
    # ç¡®ä¿æ–‡ä»¶åæœ‰ .gz åç¼€
    if not filename.endswith('.gz'):
        s3_path_gz = f"{s3_path_template}/{filename}.gz"
    else:
        s3_path_gz = f"{s3_path_template}/{filename}"
    
    # å‹ç¼©æ•°æ®
    bio = io.BytesIO()
    with gzip.GzipFile(fileobj=bio, mode='wb') as f:
        f.write(data if isinstance(data, bytes) else data.encode('utf-8'))
    compressed_data = bio.getvalue()
    
    try:
        cfg = get_s3_config()
        aws_key = cfg.get('aws_key')
        aws_secret = cfg.get('aws_secret')
        bucket = cfg.get('bucket')
        
        if not all([aws_key, aws_secret, bucket]):
            raise ValueError(f"Incomplete S3 config for {env_mode} mode")
        
        session = boto3.Session(
            aws_access_key_id=aws_key,
            aws_secret_access_key=aws_secret,
        )
        s3 = session.resource('s3')
        
        print(f"ğŸ“¤ Uploading to s3://{bucket}/{s3_path_gz} [{env_mode.upper()}]")
        s3.Bucket(bucket).put_object(Key=s3_path_gz, Body=compressed_data)
        logging.info(f"âœ… Successfully uploaded to s3://{bucket}/{s3_path_gz}")
        
    except Exception as e:
        error_msg = f"Failed to upload data to S3: {e}"
        logging.error(error_msg)
        raise RuntimeError(error_msg)


# ============================================================================
# æŠ¥å‘Šè·å–ä¸ä¿å­˜ç›¸å…³å‡½æ•°
# ============================================================================

def fetch_report(ad_network: str, ad_type: str, exc_ds: str, start_ds=None, end_ds=None, report_ds=None, custom=None, **req_opt):
    """è·å–æŠ¥å‘Šå¹¶ä¿å­˜"""
    req_opt['timeout'] = req_opt.get('timeout', 1800)
    req_opt['stream'] = True
    
    resp = requests.get(**req_opt)
    
    if resp.status_code not in [200, 204, 422]:
        raise RuntimeError(
            f'Failed to download {ad_network} report for {exc_ds}: {resp.status_code} {resp.text[:200]}'
        )
    
    if resp.status_code == 204:
        return save_report(
            ad_network=ad_network, ad_type=ad_type, report_content=b"",
            exc_ds=exc_ds, start_ds=start_ds, end_ds=end_ds, report_ds=report_ds, custom=custom
        )
    
    return save_report(
        ad_network=ad_network, ad_type=ad_type, response=resp,
        exc_ds=exc_ds, start_ds=start_ds, end_ds=end_ds, report_ds=report_ds, custom=custom
    )


def save_report(
    ad_network: str, 
    ad_type: str, 
    report=None, 
    response=None, 
    report_content=None, 
    exc_ds=None, 
    start_ds=None, 
    end_ds=None, 
    report_ds=None, 
    custom=None,
    data_format=None
):
    """
    ä¿å­˜æŠ¥å‘Šæ•°æ®å¹¶æ ¹æ®ç¯å¢ƒæ¨¡å¼è‡ªåŠ¨å¤„ç†ä¸Šä¼ 
    
    è‡ªåŠ¨è¯†åˆ«æ•°æ®æ ¼å¼ï¼šCSV, JSON, JSONL, API å“åº”ï¼ˆå¦‚ {"code":200,"results":[...]}ï¼‰
    
    æ–‡ä»¶åç”Ÿæˆè§„åˆ™: {ad_network}_{date_range}[_{custom}]
    """
    env_mode = get_env_mode()
    
    # --- [Filename Generation Logic] ---
    # 1. ç¡®å®šæ—¥æœŸéƒ¨åˆ†
    if start_ds and end_ds:
        date_part = f"{start_ds}_to_{end_ds}"
    elif report_ds:
        date_part = f"{report_ds}"
    else:
        date_part = f"{exc_ds}"
        
    # 2. æ‹¼æ¥åŸºç¡€æ–‡ä»¶å: channel_YYYY-mm-dd_to_YYYY-mm-dd
    filename_base = f"{ad_network}_{date_part}"
    
    # 3. å¦‚æœæœ‰ custom (account_id)ï¼Œè¿½åŠ åˆ°æœ€å: ...[_account_id]
    if custom:
        filename = f"{filename_base}_{custom}"
    else:
        filename = filename_base
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºæµå¼æ•°æ®ï¼ˆresponse å¯¹è±¡ï¼‰
    is_streaming = response is not None
    
    if is_streaming:
        return _save_report_streaming(
            ad_network=ad_network,
            ad_type=ad_type,
            response=response,
            filename=filename,
            exc_ds=exc_ds,
            env_mode=env_mode
        )
    
    # === éæµå¼æ•°æ®å¤„ç† ===
    
    # æ”¶é›†åŸå§‹æ•°æ®
    if report_content is not None:
        raw_data = report_content if isinstance(report_content, bytes) else report_content.encode('utf-8')
    elif report:
        raw_data = report if isinstance(report, bytes) else report.encode('utf-8')
    else:
        raw_data = b''
    
    if not raw_data:
        logging.warning("âš ï¸ No data to save")
        return None
    
    # ä½¿ç”¨ data_parser æ¨¡å—è½¬æ¢ä¸º JSONL
    try:
        text_data = raw_data.decode('utf-8')
        
        # è½¬æ¢ data_format å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
        fmt = None
        if data_format:
            try:
                fmt = DataFormat(data_format)
            except ValueError:
                fmt = None
        
        jsonl_content, row_count, detected_format = convert_to_jsonl(text_data, fmt)
        
        if detected_format == DataFormat.UNKNOWN:
            logging.warning("âš ï¸ Could not convert to JSONL, saving as original")
            jsonl_content = text_data
        else:
            print(f"âœ… Converted {detected_format.value} to JSONL format ({row_count} rows)")
        
    except Exception as e:
        logging.error(f"âŒ Error converting data: {e}")
        raise
    
    # éªŒè¯ JSONL æ ¼å¼
    if jsonl_content:
        first_line = jsonl_content.split('\n')[0].strip()
        if first_line:
            try:
                json.loads(first_line)
            except json.JSONDecodeError as e:
                logging.error(f"âŒ Invalid JSONL format after conversion: {e}")
                logging.error(f"   First 200 chars: {first_line[:200]}")
                raise ValueError(f"Invalid JSONL format: {e}")
    
    # æ ¹æ®ç¯å¢ƒæ¨¡å¼å¤„ç†
    upload_data = jsonl_content.encode('utf-8')
    
    if env_mode == 'dev':
        # dev: ä¿å­˜å®Œæ•´æ•°æ®åˆ°æœ¬åœ°
        if _DATA_BASE_PATH is None:
            raise ValueError("DATA_BASE_PATH not set for dev mode")
        
        file_path = f'{_DATA_BASE_PATH}/{ad_type}/{ad_network}/{exc_ds}/'
        os.makedirs(file_path, exist_ok=True)
        
        full_path = f'{file_path}{filename}.jsonl'
        with open(full_path, 'w', encoding='utf-8') as f:
            f.write(jsonl_content)
        
        print(f"âœ… Saved complete data to: {full_path} ({len(upload_data)} bytes)")
        return full_path
    
    elif env_mode == 'staging':
        # staging: ä¿å­˜ 5MB é¢„è§ˆåˆ°æœ¬åœ° + ä¸Šä¼ å®Œæ•´æ•°æ®åˆ° S3
        if _DATA_BASE_PATH is None:
            raise ValueError("DATA_BASE_PATH not set for staging mode")
        
        file_path = f'{_DATA_BASE_PATH}/{ad_type}/{ad_network}/{exc_ds}/'
        os.makedirs(file_path, exist_ok=True)
        
        preview_path = f'{file_path}{filename}.preview'
        
        # æŒ‰è¡Œæˆªæ–­ä¿å­˜ preview
        preview_size, preview_rows = _save_preview_by_lines(jsonl_content, preview_path)
        print(f"âœ… Saved preview: {preview_path} ({preview_size} bytes, {preview_rows} rows)")
        
        # ä¸Šä¼ å®Œæ•´æ•°æ®åˆ° S3
        s3_subpath = f"{ad_type}/{ad_network}"
        upload_data_to_s3(upload_data, s3_subpath, exc_ds, filename)
        
        return preview_path
    
    else:  # prod
        # prod: ç›´æ¥ä¸Šä¼ å®Œæ•´æ•°æ®åˆ° S3
        s3_subpath = f"{ad_type}/{ad_network}"
        upload_data_to_s3(upload_data, s3_subpath, exc_ds, filename)
        return None


def _save_report_streaming(ad_network: str, ad_type: str, response, filename: str, exc_ds: str, env_mode: str):
    """
    æµå¼å¤„ç†å¤§æ–‡ä»¶ï¼Œè‡ªåŠ¨è¯†åˆ«æ•°æ®æ ¼å¼ï¼ˆCSV/JSON/JSONL/APIå“åº”ï¼‰
    """
    import tempfile
    import shutil
    
    s3_subpath = f"{ad_type}/{ad_network}"
    
    s3_config = None
    if env_mode in ['staging', 'prod']:
        s3_config = get_s3_config()
        if not s3_config:
            raise ValueError(f"Cannot get S3 config for {env_mode} mode")
    
    local_file = None
    preview_file = None
    
    if _DATA_BASE_PATH is None and env_mode in ['dev', 'staging']:
        raise ValueError("DATA_BASE_PATH not set")
    
    if env_mode in ['dev', 'staging']:
        file_path = f'{_DATA_BASE_PATH}/{ad_type}/{ad_network}/{exc_ds}/'
        os.makedirs(file_path, exist_ok=True)
        
        if env_mode == 'dev':
            local_file = f'{file_path}{filename}.jsonl'
        elif env_mode == 'staging':
            preview_file = f'{file_path}{filename}.preview'
    
    s3_bucket = None
    s3_key = None
    s3_client = None
    
    if env_mode in ['staging', 'prod']:
        s3_path_template = build_s3_path(s3_subpath, exc_ds)
        s3_key = f"{s3_path_template}/{filename}.gz"
        
        session = boto3.Session(
            aws_access_key_id=s3_config['aws_key'],
            aws_secret_access_key=s3_config['aws_secret'],
        )
        s3_client = session.client('s3')
        s3_bucket = s3_config['bucket']
        print(f"ğŸ“¤ Will upload to s3://{s3_bucket}/{s3_key} [{env_mode.upper()}]")
    
    raw_temp_file = tempfile.TemporaryFile(mode='w+b')
    s3_temp_file = None
    s3_gzip_file = None
    local_f = None
    preview_lines = []
    preview_size = 0
    max_preview_size = 5 * 1024 * 1024
    
    try:
        # 1. ä¸‹è½½å“åº”å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶
        print("â¬‡ï¸  Downloading stream to temporary file...")
        response.raw.decode_content = True
        shutil.copyfileobj(response.raw, raw_temp_file)
        raw_temp_file.seek(0)
        print("âœ… Download complete.")
        
        # 2. ä½¿ç”¨ StreamingParser è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¹¶è§£æ
        parser = StreamingParser(chunk_size=10000)
        data_format = parser.detect_format_from_file(raw_temp_file)
        raw_temp_file.seek(0)
        
        if local_file:
            local_f = open(local_file, 'w', encoding='utf-8')
        
        if s3_bucket:
            s3_temp_file = tempfile.TemporaryFile(mode='w+b')
            s3_gzip_file = gzip.GzipFile(fileobj=s3_temp_file, mode='wb')
        
        print(f"â³ Starting data parsing and processing...")
        total_rows = 0
        chunk_count = 0
        
        # 3. ä½¿ç”¨ StreamingParser æµå¼è§£æ
        for records, batch_size in parser.parse_file(raw_temp_file, data_format):
            chunk_count += 1
            if chunk_count % 10 == 1:
                print(f"   Processing chunk {chunk_count} (rows so far: {total_rows})...")
            
            # è½¬æ¢è®°å½•ä¸º JSONL è¡Œ
            chunk_lines = []
            for record in records:
                line = json.dumps(record, ensure_ascii=False)
                chunk_lines.append(line)
                
                # æ”¶é›† preview è¡Œ
                if preview_size < max_preview_size:
                    line_size = len((line + '\n').encode('utf-8'))
                    if preview_size + line_size <= max_preview_size:
                        preview_lines.append(line)
                        preview_size += line_size
            
            chunk_jsonl = '\n'.join(chunk_lines) + '\n'
            total_rows += batch_size
            
            # å†™å…¥æœ¬åœ°å®Œæ•´æ–‡ä»¶
            if local_f:
                local_f.write(chunk_jsonl)
            
            # å†™å…¥ S3 å‹ç¼©æµ
            if s3_gzip_file:
                s3_gzip_file.write(chunk_jsonl.encode('utf-8'))
        
        print(f"âœ… Data parsing complete. Total rows: {total_rows}")
        
        # ä¿å­˜ preview æ–‡ä»¶
        if preview_file:
            with open(preview_file, 'w', encoding='utf-8') as pf:
                pf.write('\n'.join(preview_lines))
            print(f"âœ… Saved preview: {preview_file} ({preview_size} bytes, {len(preview_lines)} rows)")
        
        # å®Œæˆ S3 ä¸Šä¼ 
        if s3_gzip_file:
            s3_gzip_file.close()
            s3_gzip_file = None
            s3_temp_file.seek(0)
            
            print(f"ğŸ“¤ Uploading to S3...")
            s3_client.upload_fileobj(
                Fileobj=s3_temp_file,
                Bucket=s3_bucket,
                Key=s3_key
            )
            logging.info(f"âœ… Successfully uploaded to s3://{s3_bucket}/{s3_key}")
        
        print(f"âœ… Processed {total_rows} rows (streaming mode)")
        
    finally:
        if local_f:
            local_f.close()
        if s3_gzip_file:
            try:
                s3_gzip_file.close()
            except:
                pass
        if s3_temp_file:
            s3_temp_file.close()
        if raw_temp_file:
            raw_temp_file.close()
    
    if env_mode == 'dev':
        print(f"âœ… Saved complete data to: {local_file}")
        return local_file
    elif env_mode == 'staging':
        return preview_file
    else:
        return None


# ============================================================================
# é€šçŸ¥ç›¸å…³å‡½æ•°
# ============================================================================

def send_feishu(bot_access_token, title, infos):
    """ç®€åŒ–çš„é£ä¹¦å‘é€"""
    content_text = "\n".join(infos)
    print(f"--- [MOCKED FEISHU NOTIFICATION] ---")
    print(f"Title: {title}")
    print(f"Content: {content_text}")
    print(f"Token (Hidden): {bot_access_token[:5]}...")
    print(f"------------------------------------")
    return


def failure_callback(exception_msg, job_name):
    """Databricks ä¸“ç”¨çš„å¤±è´¥å›è°ƒ"""
    try:
        secret_env = get_cfg('env')
        feishu_botid = secret_env.get('feishu_botid') if secret_env else None
        if feishu_botid:
            send_feishu(
                feishu_botid,
                'DATABRICKS JOB FAILURE',
                [
                    f'**JOB:** {job_name}',
                    f'**ERROR:** {exception_msg}'
                ]
            )
        else:
            print(f"âš ï¸ Cannot send failure notification: Missing config")
            print(f"Job Failed: {job_name}")
            print(f"Error: {exception_msg}")
    except Exception as e:
        print(f"âš ï¸ Failed to send failure notification: {e}")
        print(f"Job Failed: {job_name}")
        print(f"Error: {exception_msg}")


# ============================================================================
# æ•°æ®éªŒè¯ä¸é¢„è§ˆç›¸å…³å‡½æ•°
# ============================================================================

def validate_and_preview_data(ad_type: str, ad_network: str):
    """
    åœ¨ staging æ¨¡å¼ä¸‹æ‰«æå¹¶é¢„è§ˆ preview æ–‡ä»¶
    
    è¯¥å‡½æ•°ä¼šï¼š
    1. æ‰«ææŒ‡å®š ad_type å’Œ ad_network ä¸‹çš„æ‰€æœ‰ .preview æ–‡ä»¶
    2. æ‰‹åŠ¨é€è¡Œè¯»å– JSONL æ ¼å¼çš„ preview æ–‡ä»¶ï¼ˆé¿å… Spark Arrow ç±»å‹è½¬æ¢é—®é¢˜ï¼‰
    3. æ˜¾ç¤ºé¢„è§ˆæ•°æ®çš„å‰ 5 è¡Œ
    
    Args:
        ad_type: å¹¿å‘Šç±»å‹ï¼Œå¦‚ 'spend', 'income', 'iap'
        ad_network: å¹¿å‘Šç½‘ç»œåç§°ï¼Œå¦‚ 'aarki', 'applovin'
    
    Returns:
        Noneï¼ˆä»…ç”¨äºæ‰“å°é¢„è§ˆä¿¡æ¯ï¼‰
    """
    env_mode = get_env_mode()
    
    if env_mode != 'staging':
        print("âš ï¸ é staging æ¨¡å¼ï¼Œè·³è¿‡æœ¬åœ° previewã€‚")
        return
    
    try:
        # ç›´æ¥ä½¿ç”¨æ¨¡å—çº§åˆ«çš„ _DATA_BASE_PATH å˜é‡
        base_root = _DATA_BASE_PATH or os.path.join(os.getcwd(), "data_output")
        preview_root = os.path.join(base_root, ad_type, ad_network)
        print(f"ğŸ” Scanning preview files under: {preview_root}")
        
        if not os.path.exists(preview_root):
            print(f"âš ï¸ Preview directory does not exist: {preview_root}")
            return
        
        # æŸ¥æ‰¾æ‰€æœ‰ .preview æ–‡ä»¶
        preview_files = []
        for root, dirs, files in os.walk(preview_root):
            for name in files:
                if name.endswith('.preview'):
                    preview_files.append(os.path.join(root, name))
        
        print(f"âœ… Found {len(preview_files)} preview file(s)")
        
        # é¢„è§ˆæ¯ä¸ªæ–‡ä»¶
        for sample_file in preview_files:
            print(f"\n   Previewing: {sample_file}")
            try:
                # æ‰‹åŠ¨é€è¡Œè¯»å– JSONLï¼Œé¿å… Spark Arrow ç±»å‹è½¬æ¢é—®é¢˜
                records = []
                with open(sample_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            try:
                                records.append(json.loads(line))
                            except json.JSONDecodeError as je:
                                print(f"   âš ï¸  Skipping invalid JSON line: {je}")
                
                if records:
                    df = pandas.DataFrame(records)
                    try:
                        display(df.head(5))
                    except NameError:
                        print(df.head(5).to_string())
                    print(f"   Total rows: {len(df)}\n")
                else:
                    print(f"   âš ï¸  No valid records found in preview file\n")
            except Exception as e:
                print(f"   âŒ Failed to read preview file: {e}")
    except Exception as e:
        print(f"âŒ Preview scan error: {e}")
