import base64
import gzip
import io
import json
import os
import logging
import requests
import pandas

# å‡è®¾ databricks ç¯å¢ƒä¸­æœ‰ boto3
import boto3

# å¼•å…¥é€‚é…åçš„é…ç½®ç®¡ç†
from .config_manager import get_s3_config, get_secret_config, get_env_mode, build_s3_path

_AD_TYPE_INCOME = "income"
_AD_TYPE_SPEND = "spend"
_AD_TYPE_IAP = "iap"
_AD_TYPE_SPEND_MONITOR = "spend_monitor"
_AD_TYPE_ATTRIBUTE = "attribution"
_DATA_BASE_PATH = None

# æ ¹æ®ç¯å¢ƒæ¨¡å¼ç¡®å®šæ•°æ®æ ¹ç›®å½•
# æ³¨æ„ï¼šåœ¨ Databricks Runtime 11.3+ ä¸­ï¼Œ/Volumes/ æ˜¯æ¨èçš„å­˜å‚¨ä½ç½®ï¼Œæ”¯æŒå¤§æ–‡ä»¶
# ä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬å…ˆä½¿ç”¨ Workspace è·¯å¾„ï¼Œä½†è¦æ³¨æ„æ–‡ä»¶å¤§å°é™åˆ¶
_env_mode = get_env_mode()
# if _env_mode == 'prod':
#     # Prod æ¨¡å¼ä¸‹é€šå¸¸ä¸éœ€è¦æœ¬åœ°è½åœ°æ–‡ä»¶ï¼Œé™¤éä¸ºäº†è°ƒè¯•
#     # å¦‚æœä¸ºäº†ç»Ÿä¸€é€»è¾‘ï¼Œå¯ä»¥è®¾ç½®ä¸º Workspace è·¯å¾„
#     _DATA_BASE_PATH = None 
# else:
_DATA_BASE_PATH = os.path.join(os.getcwd(), "data_output")

def get_cfg(cfg_name: str):
    """
    è·å–é…ç½®ï¼Œä¼˜å…ˆä» Databricks Secrets è·å–ï¼Œå…¶æ¬¡ç¯å¢ƒå˜é‡ï¼Œæœ€å variables.json
    """
    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯ 'env' é…ç½®ï¼ŒåŒ…å«äº† botid ç­‰ä¿¡æ¯
    if cfg_name == 'env':
        return get_secret_config('env')

    # å°è¯•ç›´æ¥è·å–åŒå secret (ä¾‹å¦‚ 'appsflyer')
    return get_secret_config(cfg_name)

def upload_data_to_s3(data: bytes, s3_subpath: str, exc_ds: str = None, filename: str = None):
    """
    ç›´æ¥ä»å†…å­˜æ•°æ®ä¸Šä¼ åˆ° S3ï¼ˆå‹ç¼©ä¸º Gzipï¼‰

    Args:
        data: åŸå§‹æ•°æ®ï¼ˆbytesï¼‰
        s3_subpath: S3 å­è·¯å¾„ï¼Œå¦‚ 'spend/aarki', 'iap/amazon'
        exc_ds: æ‰§è¡Œæ—¥æœŸ (YYYY-MM-DD)
        filename: å¯é€‰çš„æ–‡ä»¶å
    """
    if not data:
        logging.warning("âš ï¸ No data to upload")
        return

    env_mode = get_env_mode()

    # dev æ¨¡å¼ä¸ä¸Šä¼  S3
    if env_mode == 'dev':
        logging.info(f"ğŸ”§ [DEV MODE] Skip uploading data to S3")
        return

    # æ„å»º S3 è·¯å¾„ï¼ˆä½¿ç”¨æ–°çš„ build_s3_path å‡½æ•°ï¼‰
    s3_path_template = build_s3_path(s3_subpath, exc_ds)

    # ç”Ÿæˆæ–‡ä»¶å
    if not filename:
        filename = f"{s3_subpath.replace('/', '_')}_{exc_ds}.jsonl"

    s3_path = f"{s3_path_template}/{filename}"
    s3_path_gz = s3_path + '.gz'

    # å‹ç¼©æ•°æ®
    bio = io.BytesIO()
    with gzip.GzipFile(fileobj=bio, mode='wb') as f:
        f.write(data)
    compressed_data = bio.getvalue()

    try:
        cfg = get_s3_config()
        aws_key = cfg.get('aws_key')
        aws_secret = cfg.get('aws_secret')
        bucket = cfg.get('bucket')

        if not all([aws_key, aws_secret, bucket]):
            raise ValueError(f"Incomplete S3 config for {env_mode} mode")

        session = boto3.Session(
            aws_access_key_id=aws_key,
            aws_secret_access_key=aws_secret,
        )
        s3 = session.resource('s3')
        print(f"ğŸ“¤ Uploading to s3://{bucket}/{s3_path_gz} [{env_mode.upper()}]")

        s3.Bucket(bucket).put_object(Key=s3_path_gz, Body=compressed_data)
        logging.info(f"âœ… Successfully uploaded to s3://{bucket}/{s3_path_gz}")

    except Exception as e:
        error_msg = f"Failed to upload data to S3: {e}"
        logging.error(error_msg)
        raise RuntimeError(error_msg)

def _get_s3_path(file_path: str, dag_id: str = None, exc_ds: str = None):
    """
    (å·²å¼ƒç”¨) æ—§çš„ S3 è·¯å¾„ç”Ÿæˆé€»è¾‘ï¼Œä»…ä½œ fallback
    """
    # ç®€å•ä¿ç•™åŸæœ‰é€»è¾‘ä½œä¸º fallbackï¼Œå®é™…åº”è¯¥éƒ½èµ° dag_id_to_s3_paths.json
    relative_path = file_path.replace(_DATA_BASE_PATH, "").lstrip("/")
    # ç®€å•æ›¿æ¢ï¼šæŠŠ data_output è·¯å¾„è½¬ä¸º reports è·¯å¾„
    # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…å·²è¢« upload_data_to_s3 å–ä»£
    return f"reports/{relative_path}"
    
def _get_read_csv_error_handling_kwargs():
    """æ ¹æ® Pandas ç‰ˆæœ¬è¿”å›æ­£ç¡®çš„é”™è¯¯å¤„ç†å‚æ•°"""
    try:
        pandas_version = tuple(map(int, pandas.__version__.split('.')[:2]))
        if pandas_version >= (1, 3):
            return {'on_bad_lines': 'skip'}
        else:
            return {'error_bad_lines': False}
    except:
        # é»˜è®¤ä½¿ç”¨æ–°ç‰ˆå‚æ•°
        return {'on_bad_lines': 'skip'}

def convert_df_to_jsonl(df):
    """
    å°† DataFrame è½¬æ¢ä¸ºå¤šè¡Œ JSON (JSONL) æ ¼å¼
    
    Args:
        df: pandas DataFrame
    
    Returns:
        bytes: JSONL æ ¼å¼çš„æ•°æ®ï¼ˆUTF-8 ç¼–ç ï¼‰
    """
    # ç¡®ä¿æ—¥æœŸç›¸å…³åˆ—ä¿æŒä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼ˆé¿å… to_json è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼‰
    date_columns = ['date', 'report_date', 'start_ds', 'end_ds', 'exc_ds']
    for col in date_columns:
        if col in df.columns:
            df[col] = df[col].astype(str)
    
    # ä½¿ç”¨ json.dumps æ‰‹åŠ¨æ„å»º JSONLï¼Œç¡®ä¿æ—¥æœŸå­—æ®µä¿æŒä¸ºå­—ç¬¦ä¸²
    lines = []
    for _, row in df.iterrows():
        record = row.to_dict()
        for col in date_columns:
            if col in record:
                record[col] = str(record[col])
        lines.append(json.dumps(record, ensure_ascii=False))
    
    return '\n'.join(lines).encode('utf-8')

def fetch_report(ad_network: str, ad_type: str, exc_ds: str, start_ds=None, end_ds=None, report_ds=None, custom=None, **req_opt):
    req_opt['timeout'] = req_opt.get('timeout', 1800)
    
    # å¼€å¯æµå¼ä¸‹è½½ï¼Œé˜²æ­¢å¤§æ–‡ä»¶æ’‘çˆ†å†…å­˜
    req_opt['stream'] = True
    
    resp = requests.get(**req_opt)
    
    if resp.status_code not in [200, 204, 422]:
        raise RuntimeError(
            f'Failed to download {ad_network} report for {exc_ds}: {resp.status_code} {resp.text[:200]}' # åªæ‰“å°å‰200å­—ç¬¦ï¼Œé˜²æ­¢æŠ¥é”™ä¿¡æ¯è¿‡é•¿
        )
    
    # å¦‚æœæ˜¯ 204 (No Content)ï¼Œç›´æ¥è¿”å›ç©ºæ–‡ä»¶
    if resp.status_code == 204:
        return save_report(ad_network=ad_network, ad_type=ad_type, report_content=b"", exc_ds=exc_ds, start_ds=start_ds, end_ds=end_ds, report_ds=report_ds, custom=custom)

    # ç›´æ¥ä¼ é€’ response å¯¹è±¡ç»™ save_report è¿›è¡Œæµå¼å†™å…¥
    return save_report(ad_network=ad_network, ad_type=ad_type, response=resp, exc_ds=exc_ds, start_ds=start_ds, end_ds=end_ds, report_ds=report_ds, custom=custom)

def save_report(ad_network: str, ad_type: str, report=None, response=None, report_content=None, exc_ds=None, start_ds=None, end_ds=None, report_ds=None, custom=None):
    """
    ä¿å­˜æŠ¥å‘Šæ•°æ®å¹¶æ ¹æ®ç¯å¢ƒæ¨¡å¼è‡ªåŠ¨å¤„ç†ä¸Šä¼ ï¼ˆæ”¯æŒæµå¼å¤„ç†å¤§æ–‡ä»¶ï¼‰ï¼š

    - dev: ä¿å­˜å®Œæ•´æ•°æ®åˆ°æœ¬åœ°ï¼Œä¸ä¸Šä¼  S3
    - staging: ä¿å­˜ 5MB é¢„è§ˆåˆ°æœ¬åœ°ï¼Œå®Œæ•´æ•°æ®ä¸Šä¼  S3
    - prod: ä¸ä¿å­˜æœ¬åœ°ï¼Œå®Œæ•´æ•°æ®ç›´æ¥ä¸Šä¼  S3
    """
    env_mode = get_env_mode()

    # ç”Ÿæˆæ–‡ä»¶å
    if not report_ds and not custom:
        filename = f"{ad_network}_{start_ds}_to_{end_ds}"
    elif report_ds:
        filename = f"{ad_network}_{report_ds}"
    elif custom:
        filename = f"{ad_network}_{custom}_{start_ds}_to_{end_ds}"

    # åˆ¤æ–­æ˜¯å¦ä¸ºæµå¼æ•°æ®ï¼ˆresponse å¯¹è±¡ï¼‰
    is_streaming = response is not None
    
    # å¯¹äºæµå¼æ•°æ®ï¼ˆå¤§æ–‡ä»¶ï¼‰ï¼Œä½¿ç”¨åˆ†å—å¤„ç†é¿å… OOM
    if is_streaming:
        return _save_report_streaming(
            ad_network=ad_network,
            ad_type=ad_type,
            response=response,
            filename=filename,
            exc_ds=exc_ds,
            env_mode=env_mode
        )
    
    # å¯¹äºéæµå¼æ•°æ®ï¼ˆå°æ–‡ä»¶ï¼‰ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
    upload_data = b''
    
    try:
        # æ”¶é›†åŸå§‹æ•°æ®
        if report_content is not None:
            raw_data = report_content
        elif report:
            raw_data = report.encode('utf-8')
        else:
            raw_data = b''

        # è½¬æ¢ä¸º JSONL
        if raw_data:
            try:
                from io import StringIO
                text_data = raw_data.decode('utf-8')
                
                # å°è¯•æ£€æµ‹æ˜¯å¦ä¸º JSON æ ¼å¼ï¼ˆä»¥ [ æˆ– { å¼€å¤´ï¼‰
                is_json = False
                text_stripped = text_data.strip()
                if text_stripped.startswith('[') or text_stripped.startswith('{'):
                    try:
                        # å°è¯•è§£æä¸º JSON
                        json_data = json.loads(text_data)
                        if isinstance(json_data, list):
                            # JSON æ•°ç»„ï¼šè½¬æ¢ä¸º DataFrame å†è½¬ JSONL
                            df = pandas.DataFrame(json_data)
                            is_json = True
                        elif isinstance(json_data, dict):
                            # å•ä¸ª JSON å¯¹è±¡ï¼šè½¬æ¢ä¸ºå•è¡Œ DataFrame
                            df = pandas.DataFrame([json_data])
                            is_json = True
                    except (json.JSONDecodeError, ValueError):
                        pass  # ä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼Œç»§ç»­å°è¯• CSV
                
                if not is_json:
                    # å°è¯•ä½œä¸º CSV è¯»å–
                    csv_io = StringIO(text_data)
                    try:
                        df = pandas.read_csv(csv_io, on_bad_lines='skip')
                    except TypeError:
                        # Pandas < 1.3.0 ä¸æ”¯æŒ on_bad_lines
                        csv_io.seek(0)  # é‡ç½®è¯»å–ä½ç½®
                        df = pandas.read_csv(csv_io, error_bad_lines=False)
                
                # ç¡®ä¿æ—¥æœŸåˆ—ä¿æŒä¸ºå­—ç¬¦ä¸²
                date_columns = ['date', 'report_date', 'start_ds', 'end_ds', 'exc_ds']
                for col in date_columns:
                    if col in df.columns:
                        df[col] = df[col].astype(str)
                
                upload_data = convert_df_to_jsonl(df)
                format_type = "JSON" if is_json else "CSV"
                print(f"âœ… Converted {format_type} to JSONL format ({len(df)} rows)")
            except Exception as e:
                logging.warning(f"âš ï¸ Failed to convert to JSONL: {e}, saving as original")
                upload_data = raw_data

    except Exception as e:
        logging.error(f"âŒ Error processing data: {e}")
        raise

    # æ ¹æ®ç¯å¢ƒæ¨¡å¼å¤„ç†
    if env_mode == 'dev':
        # dev: ä¿å­˜å®Œæ•´æ•°æ®åˆ°æœ¬åœ°
        if _DATA_BASE_PATH is None:
            raise ValueError("DATA_BASE_PATH not set for dev mode")

        file_path = f'{_DATA_BASE_PATH}/{ad_type}/{ad_network}/{exc_ds}/'
        if not os.path.exists(file_path):
            os.makedirs(file_path)

        full_path = f'{file_path}{filename}'
        print(f"Saving to: {full_path}")

        with open(full_path, 'wb') as f:
            f.write(upload_data)
        print(f"âœ… Saved complete data ({len(upload_data)} bytes)")
        return full_path

    elif env_mode == 'staging':
        # staging: ä¿å­˜ 5MB é¢„è§ˆåˆ°æœ¬åœ° + ä¸Šä¼ å®Œæ•´æ•°æ®åˆ° S3
        if _DATA_BASE_PATH is None:
            raise ValueError("DATA_BASE_PATH not set for staging mode")

        file_path = f'{_DATA_BASE_PATH}/{ad_type}/{ad_network}/{exc_ds}/'
        if not os.path.exists(file_path):
            os.makedirs(file_path)

        preview_path = f'{file_path}{filename}.preview'
        print(f"Saving preview to: {preview_path}")

        # ä¿å­˜ 5MB é¢„è§ˆ
        preview_size = min(5 * 1024 * 1024, len(upload_data))
        with open(preview_path, 'wb') as f:
            f.write(upload_data[:preview_size])
        print(f"âœ… Saved preview ({preview_size} bytes)")

        # åŒæ—¶ä¸Šä¼ å®Œæ•´æ•°æ®åˆ° S3
        s3_subpath = f"{ad_type}/{ad_network}"  # ä¾‹å¦‚: spend/aarki, iap/amazon
        upload_data_to_s3(upload_data, s3_subpath, exc_ds, filename)

        # è¿”å›æœ¬åœ° preview è·¯å¾„ï¼Œä¾¿äºä¸Šæ¸¸åšæœ¬åœ°é¢„è§ˆ
        return preview_path

    else:  # prod
        # prod: ç›´æ¥ä¸Šä¼ å®Œæ•´æ•°æ®åˆ° S3
        s3_subpath = f"{ad_type}/{ad_network}"  # ä¾‹å¦‚: spend/aarki, iap/amazon
        upload_data_to_s3(upload_data, s3_subpath, exc_ds, filename)
        return None

def _save_report_streaming(ad_network: str, ad_type: str, response, filename: str, exc_ds: str, env_mode: str):
    """
    æµå¼å¤„ç†å¤§æ–‡ä»¶ï¼š
    1. å…ˆå°†åŸå§‹æ•°æ®æµå¼ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆé¿å… TextIOWrapper åŒ…è£…ç½‘ç»œæµçš„ä¸ç¨³å®šæ€§ï¼‰
    2. åˆ†å—è¯»å–ä¸´æ—¶ CSV æ–‡ä»¶ï¼Œè½¬æ¢ä¸º JSONL
    3. æµå¼å†™å…¥æœ¬åœ°/S3ï¼ˆS3 ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å­˜å‹ç¼©æ•°æ®ï¼‰
    """
    import tempfile
    import shutil
    
    # æ„å»º S3 å­è·¯å¾„
    s3_subpath = f"{ad_type}/{ad_network}"  # ä¾‹å¦‚: spend/aarki, iap/amazon
    
    # è·å– S3 é…ç½®ï¼ˆå¦‚æœéœ€è¦ä¸Šä¼ ï¼‰
    s3_config = None
    if env_mode in ['staging', 'prod']:
        s3_config = get_s3_config()
        if not s3_config:
            raise ValueError(f"Cannot get S3 config for {env_mode} mode")
    
    # å‡†å¤‡æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœéœ€è¦ä¿å­˜ï¼‰
    local_file = None
    preview_file = None
    
    if _DATA_BASE_PATH is None and env_mode in ['dev', 'staging']:
         raise ValueError("DATA_BASE_PATH not set")
         
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if env_mode in ['dev', 'staging']:
        file_path = f'{_DATA_BASE_PATH}/{ad_type}/{ad_network}/{exc_ds}/'
        if not os.path.exists(file_path):
            os.makedirs(file_path)
            
        if env_mode == 'dev':
            # Dev æ¨¡å¼ï¼šåªä¿å­˜å®Œæ•´æœ¬åœ°æ–‡ä»¶
            local_file = f'{file_path}{filename}'
        elif env_mode == 'staging':
            # Staging æ¨¡å¼ï¼šåªä¿å­˜ Preview æ–‡ä»¶
            preview_file = f'{file_path}{filename}.preview'
    
    # å‡†å¤‡ S3 ä¸Šä¼ è·¯å¾„
    s3_bucket = None
    s3_key = None
    s3_client = None
    
    if env_mode in ['staging', 'prod']:
        # ä½¿ç”¨ build_s3_path æ„å»ºè·¯å¾„
        s3_path_template = build_s3_path(s3_subpath, exc_ds)
        s3_key = f"{s3_path_template}/{filename}.gz"
        
        session = boto3.Session(
            aws_access_key_id=s3_config['aws_key'],
            aws_secret_access_key=s3_config['aws_secret'],
        )
        s3_client = session.client('s3')
        s3_bucket = s3_config['bucket']
        print(f"ğŸ“¤ Will upload to s3://{s3_bucket}/{s3_key} [{env_mode.upper()}]")
    
    # èµ„æºå¥æŸ„åˆå§‹åŒ–
    raw_temp_file = tempfile.TemporaryFile(mode='w+b') # å­˜å‚¨åŸå§‹ä¸‹è½½æ•°æ®
    s3_temp_file = None # å­˜å‚¨å‹ç¼©åçš„ä¸Šä¼ æ•°æ®
    s3_gzip_file = None
    local_f = None
    preview_f = None

    try:
        # Step 1: ä¸‹è½½åŸå§‹æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶
        # ä½¿ç”¨ shutil.copyfileobj é«˜æ•ˆä¼ è¾“ï¼Œé¿å…æ‰‹åŠ¨ chunk å¾ªç¯
        print("â¬‡ï¸  Downloading stream to temporary file...")
        response.raw.decode_content = True
        shutil.copyfileobj(response.raw, raw_temp_file)
        raw_temp_file.seek(0) # é‡ç½®æŒ‡é’ˆåˆ°æ–‡ä»¶å¼€å¤´
        print("âœ… Download complete.")

        # Step 2: å‡†å¤‡è¾“å‡ºæµ
        if local_file:
            local_f = open(local_file, 'wb')
        
        if preview_file:
            preview_f = open(preview_file, 'wb')
            
        if s3_bucket:
            s3_temp_file = tempfile.TemporaryFile(mode='w+b')
            s3_gzip_file = gzip.GzipFile(fileobj=s3_temp_file, mode='wb')

        # Step 3: åˆ†å—å¤„ç†
        print("â³ Starting CSV parsing and processing...")
        chunk_size = 10000  # å‡å°åˆ° 1ä¸‡è¡Œï¼Œæé«˜å“åº”é€Ÿåº¦
        total_rows = 0
        preview_size = 5 * 1024 * 1024  # 5MB
        preview_written = 0
        chunk_count = 0
        
        date_columns = ['date', 'report_date', 'start_ds', 'end_ds', 'exc_ds']
        
        # ä»ä¸´æ—¶æ–‡ä»¶è¯»å– CSVï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶å¥æŸ„
        pandas_version = tuple(map(int, pandas.__version__.split('.')[:2]))
        read_csv_kwargs = {'chunksize': chunk_size}
        if pandas_version >= (1, 3):
            read_csv_kwargs['on_bad_lines'] = 'skip'
        else:
            read_csv_kwargs['error_bad_lines'] = False
        for chunk_df in pandas.read_csv(raw_temp_file, **read_csv_kwargs):
            chunk_count += 1
            if chunk_count % 10 == 1: # æ¯10ä¸ªchunkæ‰“å°ä¸€æ¬¡ï¼Œé¿å…æ—¥å¿—è¿‡å¤šï¼Œä½†é¦–ä¸ªchunkä¼šæ‰“å°
                print(f"   Processing chunk {chunk_count} (rows so far: {total_rows})...")

            # ç¡®ä¿æ—¥æœŸåˆ—ä¿æŒä¸ºå­—ç¬¦ä¸²
            for col in date_columns:
                if col in chunk_df.columns:
                    chunk_df[col] = chunk_df[col].astype(str)
            
            # è½¬æ¢ä¸º JSONL
            chunk_jsonl = convert_df_to_jsonl(chunk_df)
            total_rows += len(chunk_df)
            
            # å†™å…¥æœ¬åœ°å®Œæ•´æ–‡ä»¶
            if local_f:
                local_f.write(chunk_jsonl)
            
            # å†™å…¥é¢„è§ˆæ–‡ä»¶
            if preview_f and preview_written < preview_size:
                remaining = preview_size - preview_written
                if len(chunk_jsonl) <= remaining:
                    preview_f.write(chunk_jsonl)
                    preview_written += len(chunk_jsonl)
                else:
                    preview_f.write(chunk_jsonl[:remaining])
                    preview_written = preview_size
            
            # å†™å…¥ S3 å‹ç¼©æµ
            if s3_gzip_file:
                s3_gzip_file.write(chunk_jsonl)
        
        print(f"âœ… CSV parsing complete. Total rows: {total_rows}")
        
        # Step 4: å®Œæˆå†™å…¥å¹¶ä¸Šä¼ 
        if s3_gzip_file:
            s3_gzip_file.close() # å¿…é¡»å…ˆå…³é—­ gzip ä»¥å†™å…¥ footer
            s3_gzip_file = None  # æ ‡è®°å·²å…³é—­
            s3_temp_file.seek(0) # é‡ç½®æŒ‡é’ˆ
            
            print(f"ğŸ“¤ Uploading to S3 (streaming from temp file)...")
            s3_client.upload_fileobj(
                Fileobj=s3_temp_file,
                Bucket=s3_bucket,
                Key=s3_key
            )
            logging.info(f"âœ… Successfully uploaded to s3://{s3_bucket}/{s3_key}")
        
        print(f"âœ… Processed {total_rows} rows (streaming mode)")
        
    finally:
        # å…³é—­æ‰€æœ‰æ–‡ä»¶å¥æŸ„
        if local_f: local_f.close()
        if preview_f: preview_f.close()
        if s3_gzip_file: 
            try: s3_gzip_file.close()
            except: pass
        if s3_temp_file: s3_temp_file.close() # ä¼šè‡ªåŠ¨åˆ é™¤
        if raw_temp_file: raw_temp_file.close() # ä¼šè‡ªåŠ¨åˆ é™¤
    
    # è¿”å›è·¯å¾„
    if env_mode == 'dev':
        print(f"âœ… Saved complete data to: {local_file}")
        return local_file
    elif env_mode == 'staging':
        print(f"âœ… Saved preview ({preview_written} bytes) to: {preview_file}")
        return preview_file
    else:  # prod
        return None

# ç®€åŒ–çš„é£ä¹¦å‘é€ï¼Œå»é™¤ DingDing
def send_feishu(bot_access_token, title, infos):
    # è¿ç§»æµ‹è¯•å®‰å…¨æ¨¡å¼ï¼šåªæ‰“å°æ—¥å¿—ï¼Œä¸å‘é€çœŸå® HTTP è¯·æ±‚
    # if not bot_access_token:
    #     return
    # url = f'https://open.feishu.cn/open-apis/bot/v2/hook/{bot_access_token}'

    # ç®€åŒ–ç‰ˆ content æ„é€ 
    content_text = "\n".join(infos)
    
    # æ¨¡æ‹Ÿå‘é€ï¼Œåªåœ¨ Driver æ—¥å¿—ä¸­æ‰“å°
    print(f"--- [MOCKED FEISHU NOTIFICATION] ---")
    print(f"Title: {title}")
    print(f"Content: {content_text}")
    print(f"Token (Hidden): {bot_access_token[:5]}...")
    print(f"------------------------------------")
    return

    # ä»¥ä¸‹ä¸ºçœŸå®å‘é€é€»è¾‘ï¼Œæš‚æ—¶å±è”½
    # data = {
    #     "msg_type": "text",
    #     "content": {
    #         "text": f"{title}\n\n{content_text}"
    #     }
    # }

    # try:
    #     requests.post(url, json=data)
    # except Exception as e:
    #     print(f"Feishu send error: {e}")

def failure_callback(exception_msg, job_name):
    """
    Databricks ä¸“ç”¨çš„å¤±è´¥å›è°ƒ
    """
    try:
        secret_env = get_cfg('env')
        feishu_botid = secret_env.get('feishu_botid') if secret_env else None
        if feishu_botid:
            send_feishu(
                feishu_botid, 
                'DATABRICKS JOB FAILURE',
                [
                    f'**JOB:** {job_name}',
                    f'**ERROR:** {exception_msg}'
                ]
            )
        else:
            print(f"âš ï¸ Cannot send failure notification: Missing config of env. Please set env var SECRET_ENV or secret secret_env")
            print(f"Job Failed: {job_name}")
            print(f"Error: {exception_msg}")
    except Exception as e:
        print(f"âš ï¸ Failed to send failure notification: {e}")
        print(f"Job Failed: {job_name}")
        print(f"Error: {exception_msg}")
