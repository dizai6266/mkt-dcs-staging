# Databricks Notebook å¼€å‘è§„èŒƒ

æœ¬æ–‡æ¡£å®šä¹‰äº† MKT DCS é¡¹ç›®ä¸­ Databricks Notebook çš„å¼€å‘è§„èŒƒï¼Œç¡®ä¿ä»£ç ä¸€è‡´æ€§ã€å¯ç»´æŠ¤æ€§å’Œå¯é æ€§ã€‚

---

## ç›®å½•

1. [æ•´ä½“ç»“æ„](#æ•´ä½“ç»“æ„)
2. [å„éƒ¨åˆ†è¯¦è§£](#å„éƒ¨åˆ†è¯¦è§£)
3. [å¤±è´¥å›è°ƒæœºåˆ¶](#å¤±è´¥å›è°ƒæœºåˆ¶)
4. [å‘½åè§„èŒƒ](#å‘½åè§„èŒƒ)
5. [ä»£ç é£æ ¼](#ä»£ç é£æ ¼)
6. [æ—¥å¿—è¾“å‡ºè§„èŒƒ](#æ—¥å¿—è¾“å‡ºè§„èŒƒ)
7. [æ¨¡æ¿ç¤ºä¾‹](#æ¨¡æ¿ç¤ºä¾‹)

---

## æ•´ä½“ç»“æ„

æ¯ä¸ª Notebook å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹ **5 ä¸ªéƒ¨åˆ†** çš„ç»“æ„ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  # Title & Description                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ## 1. Setup & Imports                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ## 2. Configuration                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ## 3. Task Logic                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ## 4. Execution                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ## 5. Data Validation                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å„éƒ¨åˆ†è¯¦è§£

### Part 1: Title & Description

**å¿…é¡»åŒ…å«**ï¼š
- Notebook æ ‡é¢˜ï¼ˆä½¿ç”¨ H1ï¼‰
- ç®€çŸ­çš„åŠŸèƒ½æè¿°

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # {AdNetwork} Spend Report
# MAGIC
# MAGIC è¯¥ Notebook ä» {AdNetwork} API è·å–å¹¿å‘Šæ¶ˆè€—æ•°æ®ã€‚
```

### Part 2: Setup & Imports

**å¿…é¡»åŒ…å«**ï¼š
1. æ ‡å‡†åº“å¯¼å…¥
2. ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
3. é¡¹ç›®å†…éƒ¨æ¨¡å—å¯¼å…¥
4. ç¯å¢ƒåˆå§‹åŒ–ç¡®è®¤

```python
# MAGIC %md
# MAGIC ## 1. Setup & Imports

# COMMAND ----------

import requests
import json
from datetime import datetime, timedelta
import sys
import os
import pandas as pd

# åŠ¨æ€æ·»åŠ å½“å‰ç›®å½•åˆ° sys.path ä»¥åŠ è½½ utils
current_dir = os.getcwd()
if current_dir not in sys.path:
    sys.path.append(current_dir)

from utils import helper
from utils.config_manager import get_env_mode
import importlib
importlib.reload(helper)

print(f"ğŸ”§ Environment Mode: {get_env_mode()}")
print(f"âœ… Environment Setup Complete. Current Dir: {os.getcwd()}")
```

### Part 3: Configuration

**å¿…é¡»åŒ…å«**ï¼š
1. `_AD_NETWORK` å¸¸é‡å®šä¹‰
2. `_DATE_RANGE` å¸¸é‡ï¼ˆå¦‚é€‚ç”¨ï¼‰
3. Widget å‚æ•°è·å–ï¼ˆä½¿ç”¨ try-except å…¼å®¹æœ¬åœ°è¿è¡Œï¼‰
4. å‚æ•°éªŒè¯å’Œé»˜è®¤å€¼

```python
# MAGIC %md
# MAGIC ## 2. Configuration

# COMMAND ----------

# --- [é…ç½®å‚æ•°] ---
_AD_NETWORK = 'ad_network_name'
_DATE_RANGE = 7  # æ—¥æœŸèŒƒå›´ï¼ˆå¤©ï¼‰

# è·å– Widget å‚æ•°
try:
    dbutils.widgets.text("ds", "", "Execution Date (YYYY-MM-DD)")
    ds_param = dbutils.widgets.get("ds")
except:
    ds_param = ""

if not ds_param:
    ds_param = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')

print(f"ğŸ“… Execution Date: {ds_param}")
```

### Part 4: Task Logic

**å¿…é¡»åŒ…å«**ï¼š
1. ä¸»ä»»åŠ¡å‡½æ•° `fetch_spend_report_task(ds: str)`
2. è¾…åŠ©å‡½æ•°ï¼ˆå¦‚éœ€è¦ï¼‰
3. å®Œæ•´çš„ docstring

```python
# MAGIC %md
# MAGIC ## 3. Task Logic

# COMMAND ----------

def fetch_spend_report_task(ds: str):
    """
    è·å– {AdNetwork} æ¶ˆè€—æŠ¥å‘Š
    
    Args:
        ds: æ‰§è¡Œæ—¥æœŸ (YYYY-MM-DD)
    """
    # 1. è®¡ç®—æ—¥æœŸèŒƒå›´
    end_dt = datetime.strptime(ds, '%Y-%m-%d')
    end_ds = ds
    start_dt = end_dt + timedelta(days=-(_DATE_RANGE))
    start_ds = start_dt.strftime('%Y-%m-%d')
    
    print(f"ğŸ“† Date Range: {start_ds} to {end_ds}")
    
    # 2. è·å–é…ç½®
    cfg = helper.get_cfg(_AD_NETWORK)
    
    # 3. è°ƒç”¨ API
    # ... API è°ƒç”¨é€»è¾‘ ...
    
    # 4. ä¿å­˜æŠ¥å‘Š
    helper.save_report(
        ad_network=_AD_NETWORK, 
        ad_type=helper._AD_TYPE_SPEND, 
        report=report_data, 
        exc_ds=ds, 
        start_ds=start_ds, 
        end_ds=end_ds
    )
    print(f"âœ… Saved {_AD_NETWORK} report for {start_ds} to {end_ds}")
```

### Part 5: Execution

**å¿…é¡»åŒ…å«**ï¼š
1. Job å¯åŠ¨æ—¥å¿—
2. try-except åŒ…è£¹çš„ä»»åŠ¡æ‰§è¡Œ
3. **on_failure_callback**: å¤±è´¥æ—¶è°ƒç”¨ `helper.failure_callback()`
4. é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼ˆä¿æŒ Databricks Job å¤±è´¥çŠ¶æ€ï¼‰

```python
# MAGIC %md
# MAGIC ## 4. Execution

# COMMAND ----------

print(f"ğŸš€ Starting Job for {_AD_NETWORK}")

try:
    fetch_spend_report_task(ds_param)
    print("\nâœ… Job Finished Successfully")

except Exception as e:
    print(f"\nâŒ Job Failed: {e}")
    # on_failure_callback: å¤±è´¥æ—¶å‘é€é£ä¹¦é€šçŸ¥
    helper.failure_callback(str(e), f"{_AD_NETWORK}_spend_report")
    raise e
```

### Part 6: Data Validation

**å¿…é¡»åŒ…å«**ï¼š
1. ç¯å¢ƒæ¨¡å¼æ£€æŸ¥
2. staging æ¨¡å¼ä¸‹çš„æ•°æ®é¢„è§ˆ
3. å¼‚å¸¸å¤„ç†

```python
# MAGIC %md
# MAGIC ## 5. Data Validation

# COMMAND ----------

env_mode = get_env_mode()
print(f"\nğŸ” Data Validation (ENV_MODE={env_mode})")

if env_mode != 'staging':
    print("âš ï¸ é staging æ¨¡å¼ï¼Œè·³è¿‡æœ¬åœ° previewã€‚")
else:
    try:
        base_root = getattr(helper, "_DATA_BASE_PATH", None) or os.path.join(os.getcwd(), "data_output")
        preview_root = os.path.join(base_root, helper._AD_TYPE_SPEND, _AD_NETWORK)
        print(f"ğŸ” Scanning preview files under: {preview_root}")

        if not os.path.exists(preview_root):
            print(f"âš ï¸ Preview directory does not exist: {preview_root}")
        else:
            preview_files = []
            for root, dirs, files in os.walk(preview_root):
                for name in files:
                    if name.endswith('.preview'):
                        preview_files.append(os.path.join(root, name))

            print(f"âœ… Found {len(preview_files)} preview file(s)")

            for sample_file in preview_files:
                print(f"\n   Previewing: {sample_file}")
                try:
                    df = pd.read_json(sample_file, lines=True)
                    try:
                        display(df.head(5))
                    except NameError:
                        print(df.head(5).to_string())
                    print(f"   Total rows: {len(df)}\n")
                except Exception as e:
                    print(f"   âŒ Failed to read preview file: {e}")
    except Exception as e:
        print(f"âŒ Preview scan error: {e}")
```

---

## å¤±è´¥å›è°ƒæœºåˆ¶

### åŸºæœ¬ç”¨æ³•

```python
try:
    fetch_spend_report_task(ds_param)
    print("\nâœ… Job Finished Successfully")
except Exception as e:
    print(f"\nâŒ Job Failed: {e}")
    # on_failure_callback: å¤±è´¥æ—¶å‘é€é£ä¹¦é€šçŸ¥
    helper.failure_callback(str(e), f"{_AD_NETWORK}_spend_report")
    raise e  # å¿…é¡»é‡æ–°æŠ›å‡ºï¼Œä¿æŒ Job å¤±è´¥çŠ¶æ€
```

### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `exception_msg` | `str` | å¼‚å¸¸ä¿¡æ¯æ–‡æœ¬ |
| `job_name` | `str` | Job åç§°ï¼Œå»ºè®®æ ¼å¼: `{ad_network}_spend_report` |

### é€šçŸ¥å†…å®¹

å¤±è´¥æ—¶ä¼šå‘é€é£ä¹¦é€šçŸ¥ï¼ŒåŒ…å«ï¼š
- **JOB**: Job åç§°
- **ERROR**: é”™è¯¯è¯¦æƒ…

### é«˜çº§ç”¨æ³•ï¼ˆä½¿ç”¨ feishu-notifyï¼‰

å¦‚éœ€æ›´ä¸°å¯Œçš„é€šçŸ¥åŠŸèƒ½ï¼Œå¯ä½¿ç”¨ `feishu-notify` æ¨¡å—ï¼š

```python
from feishu_notify import Notifier

notifier = Notifier(webhook="https://...", source="Databricks")

# å‘é€é”™è¯¯é€šçŸ¥
notifier.error(
    title=f"ä»»åŠ¡å¤±è´¥: {_AD_NETWORK}",
    error_msg=str(e),
    task_name=f"{_AD_NETWORK}_spend_report",
    link_url="https://databricks.com/job/xxx"
)
```

---

## å‘½åè§„èŒƒ

### æ–‡ä»¶å‘½å

| ç±»å‹ | æ ¼å¼ | ç¤ºä¾‹ |
|------|------|------|
| Spend æŠ¥å‘Š | `{ad_network}_spend_report.py` | `aarki_spend_report.py` |
| Asset æŠ¥å‘Š | `{ad_network}_asset_spend_report.py` | `applovin_asset_spend_report.py` |
| å½’å› æŠ¥å‘Š | `{ad_network}_attribution_report.py` | `appsflyer_attribution_report.py` |

### å¸¸é‡å‘½å

| å¸¸é‡ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `_AD_NETWORK` | å¹¿å‘Šç½‘ç»œæ ‡è¯†ï¼ˆå°å†™ï¼Œä¸‹åˆ’çº¿åˆ†éš”ï¼‰ | `'apple_search'` |
| `_DATE_RANGE` | æ—¥æœŸèŒƒå›´ï¼ˆå¤©æ•°ï¼‰ | `7` |
| `_AD_TYPE_*` | æŠ¥å‘Šç±»å‹ï¼ˆä» helper å¯¼å…¥ï¼‰ | `helper._AD_TYPE_SPEND` |

### å‡½æ•°å‘½å

| å‡½æ•° | è¯´æ˜ |
|------|------|
| `fetch_spend_report_task(ds)` | ä¸»ä»»åŠ¡å‡½æ•° |
| `get_xxx_token(cfg)` | è·å– Token |
| `get_xxx_info(...)` | è·å–ç‰¹å®šä¿¡æ¯ |
| `_parse_xxx_data(...)` | å†…éƒ¨è§£æå‡½æ•°ï¼ˆä¸‹åˆ’çº¿å‰ç¼€ï¼‰ |

---

## ä»£ç é£æ ¼

### é€šç”¨è§„èŒƒ

1. **ç¼©è¿›**: 4 ç©ºæ ¼
2. **è¡Œå®½**: ä¸è¶…è¿‡ 120 å­—ç¬¦
3. **ç©ºè¡Œ**: å‡½æ•°ä¹‹é—´ 2 ç©ºè¡Œï¼Œé€»è¾‘å—ä¹‹é—´ 1 ç©ºè¡Œ
4. **æ³¨é‡Š**: ä½¿ç”¨ä¸­æ–‡æ³¨é‡Šï¼Œå¤æ‚é€»è¾‘å¿…é¡»æ³¨é‡Š

### å¯¼å…¥é¡ºåº

```python
# 1. æ ‡å‡†åº“
import os
import sys
import json
from datetime import datetime, timedelta

# 2. ç¬¬ä¸‰æ–¹åº“
import requests
import pandas as pd

# 3. é¡¹ç›®å†…éƒ¨æ¨¡å—
from utils import helper
from utils.config_manager import get_env_mode
```

### å­—ç¬¦ä¸²æ ¼å¼åŒ–

ä½¿ç”¨ f-stringï¼š

```python
# âœ… æ¨è
print(f"Date: {start_ds} to {end_ds}")

# âŒ ä¸æ¨è
print("Date: {} to {}".format(start_ds, end_ds))
print("Date: %s to %s" % (start_ds, end_ds))
```

### å¼‚å¸¸å¤„ç†

```python
# âœ… æ¨èï¼šå…·ä½“å¼‚å¸¸ + ä¸Šä¸‹æ–‡ä¿¡æ¯
try:
    response = requests.get(url)
    if response.status_code != 200:
        raise RuntimeError(f"API Error: {response.status_code} {response.text[:200]}")
except Exception as e:
    print(f"âŒ Error: {e}")
    raise

# âŒ ä¸æ¨èï¼šåæ‰å¼‚å¸¸
try:
    response = requests.get(url)
except:
    pass
```

---

## æ—¥å¿—è¾“å‡ºè§„èŒƒ

### Emoji å‰ç¼€

| Emoji | å«ä¹‰ | ä½¿ç”¨åœºæ™¯ |
|-------|------|----------|
| ğŸ”§ | é…ç½® | ç¯å¢ƒé…ç½®ä¿¡æ¯ |
| âœ… | æˆåŠŸ | æ“ä½œå®Œæˆ |
| âŒ | å¤±è´¥ | é”™è¯¯å‘ç”Ÿ |
| âš ï¸ | è­¦å‘Š | éè‡´å‘½é—®é¢˜ |
| ğŸ“… | æ—¥æœŸ | æ‰§è¡Œæ—¥æœŸ |
| ğŸ“† | èŒƒå›´ | æ—¥æœŸèŒƒå›´ |
| ğŸ“¡ | è¯·æ±‚ | API è¯·æ±‚ |
| ğŸ“‹ | åˆ—è¡¨ | æ•°æ®ç»Ÿè®¡ |
| ğŸ“Š | æ•°æ® | æ•°æ®é‡ç»Ÿè®¡ |
| ğŸ”‘ | è®¤è¯ | Token è·å– |
| ğŸ” | æœç´¢ | æ–‡ä»¶æ‰«æ |
| ğŸš€ | å¯åŠ¨ | Job å¼€å§‹ |

### æ—¥å¿—æ ¼å¼

```python
# é˜¶æ®µå¼€å§‹
print(f"ğŸš€ Starting Job for {_AD_NETWORK}")

# é…ç½®ä¿¡æ¯
print(f"ğŸ“… Execution Date: {ds_param}")
print(f"ğŸ“† Date Range: {start_ds} to {end_ds}")

# API è°ƒç”¨
print(f"ğŸ“¡ Fetching report from: {url}")

# æ•°æ®ç»Ÿè®¡
print(f"ğŸ“Š Total records: {len(records)}")

# æ“ä½œå®Œæˆ
print(f"âœ… Saved {_AD_NETWORK} report")

# é”™è¯¯ä¿¡æ¯
print(f"âŒ Job Failed: {e}")

# è­¦å‘Šä¿¡æ¯
print(f"âš ï¸ No data returned")
```

---

## æ¨¡æ¿ç¤ºä¾‹

å®Œæ•´çš„ Notebook æ¨¡æ¿ï¼š

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # {AdNetwork} Spend Report
# MAGIC
# MAGIC è¯¥ Notebook ä» {AdNetwork} API è·å–å¹¿å‘Šæ¶ˆè€—æ•°æ®ã€‚

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Setup & Imports

# COMMAND ----------

import requests
import json
from datetime import datetime, timedelta
import sys
import os
import pandas as pd

# åŠ¨æ€æ·»åŠ å½“å‰ç›®å½•åˆ° sys.path ä»¥åŠ è½½ utils
current_dir = os.getcwd()
if current_dir not in sys.path:
    sys.path.append(current_dir)

from utils import helper
from utils.config_manager import get_env_mode
import importlib
importlib.reload(helper)

print(f"ğŸ”§ Environment Mode: {get_env_mode()}")
print(f"âœ… Environment Setup Complete. Current Dir: {os.getcwd()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configuration

# COMMAND ----------

# --- [é…ç½®å‚æ•°] ---
_AD_NETWORK = 'ad_network_name'
_DATE_RANGE = 7

# è·å– Widget å‚æ•°
try:
    dbutils.widgets.text("ds", "", "Execution Date (YYYY-MM-DD)")
    ds_param = dbutils.widgets.get("ds")
except:
    ds_param = ""

if not ds_param:
    ds_param = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')

print(f"ğŸ“… Execution Date: {ds_param}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Task Logic

# COMMAND ----------

def fetch_spend_report_task(ds: str):
    """
    è·å– {AdNetwork} æ¶ˆè€—æŠ¥å‘Š
    
    Args:
        ds: æ‰§è¡Œæ—¥æœŸ (YYYY-MM-DD)
    """
    end_dt = datetime.strptime(ds, '%Y-%m-%d')
    end_ds = ds
    start_dt = end_dt + timedelta(days=-(_DATE_RANGE))
    start_ds = start_dt.strftime('%Y-%m-%d')
    
    print(f"ğŸ“† Date Range: {start_ds} to {end_ds}")
    
    cfg = helper.get_cfg(_AD_NETWORK)
    # TODO: å®ç° API è°ƒç”¨é€»è¾‘
    
    # ä¿å­˜æŠ¥å‘Š
    helper.save_report(
        ad_network=_AD_NETWORK, 
        ad_type=helper._AD_TYPE_SPEND, 
        report=report_data, 
        exc_ds=ds, 
        start_ds=start_ds, 
        end_ds=end_ds
    )
    print(f"âœ… Saved {_AD_NETWORK} report for {start_ds} to {end_ds}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Execution

# COMMAND ----------

print(f"ğŸš€ Starting Job for {_AD_NETWORK}")

try:
    fetch_spend_report_task(ds_param)
    print("\nâœ… Job Finished Successfully")

except Exception as e:
    print(f"\nâŒ Job Failed: {e}")
    # on_failure_callback: å¤±è´¥æ—¶å‘é€é£ä¹¦é€šçŸ¥
    helper.failure_callback(str(e), f"{_AD_NETWORK}_spend_report")
    raise e

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Data Validation

# COMMAND ----------

env_mode = get_env_mode()
print(f"\nğŸ” Data Validation (ENV_MODE={env_mode})")

if env_mode != 'staging':
    print("âš ï¸ é staging æ¨¡å¼ï¼Œè·³è¿‡æœ¬åœ° previewã€‚")
else:
    try:
        base_root = getattr(helper, "_DATA_BASE_PATH", None) or os.path.join(os.getcwd(), "data_output")
        preview_root = os.path.join(base_root, helper._AD_TYPE_SPEND, _AD_NETWORK)
        print(f"ğŸ” Scanning preview files under: {preview_root}")

        if not os.path.exists(preview_root):
            print(f"âš ï¸ Preview directory does not exist: {preview_root}")
        else:
            preview_files = []
            for root, dirs, files in os.walk(preview_root):
                for name in files:
                    if name.endswith('.preview'):
                        preview_files.append(os.path.join(root, name))

            print(f"âœ… Found {len(preview_files)} preview file(s)")

            for sample_file in preview_files:
                print(f"\n   Previewing: {sample_file}")
                try:
                    df = pd.read_json(sample_file, lines=True)
                    try:
                        display(df.head(5))
                    except NameError:
                        print(df.head(5).to_string())
                    print(f"   Total rows: {len(df)}\n")
                except Exception as e:
                    print(f"   âŒ Failed to read preview file: {e}")
    except Exception as e:
        print(f"âŒ Preview scan error: {e}")
```

---

## Checklist

æ–°å¢ Notebook å‰ï¼Œè¯·ç¡®è®¤ä»¥ä¸‹äº‹é¡¹ï¼š

- [ ] æ–‡ä»¶åç¬¦åˆ `{ad_network}_spend_report.py` æ ¼å¼
- [ ] åŒ…å«å®Œæ•´çš„ 5 ä¸ªéƒ¨åˆ†ç»“æ„
- [ ] `_AD_NETWORK` å¸¸é‡å·²æ­£ç¡®å®šä¹‰
- [ ] ä¸»å‡½æ•° `fetch_spend_report_task(ds)` å·²å®ç°
- [ ] Execution éƒ¨åˆ†ä½¿ç”¨ try-except å¹¶è°ƒç”¨ `helper.failure_callback()`
- [ ] å¼‚å¸¸è¢«æ­£ç¡®é‡æ–°æŠ›å‡º (`raise e`)
- [ ] Data Validation éƒ¨åˆ†å·²æ·»åŠ 
- [ ] æ—¥å¿—è¾“å‡ºä½¿ç”¨è§„èŒƒçš„ Emoji å‰ç¼€
- [ ] å‡½æ•°åŒ…å«å®Œæ•´çš„ docstring

---

## æ›´æ–°è®°å½•

| æ—¥æœŸ | ç‰ˆæœ¬ | æ›´æ–°å†…å®¹ |
|------|------|----------|
| 2025-12-09 | v1.0 | åˆå§‹ç‰ˆæœ¬ |

