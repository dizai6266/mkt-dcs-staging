# Databricks notebook source
# MAGIC %md
# MAGIC # Everflow Spend Report
# MAGIC
# MAGIC è¯¥ Notebook ä» Everflow API è·å–å¹¿å‘Šæ¶ˆè€—æ•°æ®ã€‚
# MAGIC æ”¯æŒå¤šä¸ª API Keyï¼Œè¿”å› JSON æ ¼å¼æ•°æ®ã€‚

# COMMAND ----------

# MAGIC %pip install httpx

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Setup & Imports

# COMMAND ----------

import requests
import json
import logging
from datetime import datetime, timedelta
import sys
import os
import pandas as pd

# åŠ¨æ€æ·»åŠ å½“å‰ç›®å½•åˆ° sys.path ä»¥åŠ è½½ utils
current_dir = os.getcwd()
if current_dir not in sys.path:
    sys.path.append(current_dir)

from utils import helper
from utils.config_manager import get_env_mode, setup_feishu_notify
import importlib
importlib.reload(helper)

# è®¾ç½® feishu-notifyï¼ˆè·¯å¾„å·²åœ¨ config_manager ä¸­é…ç½®ï¼‰
Notifier = setup_feishu_notify()

print(f"ğŸ”§ Environment Mode: {get_env_mode()}")
print(f"âœ… Environment Setup Complete. Current Dir: {os.getcwd()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configuration

# COMMAND ----------

# --- [é…ç½®å‚æ•°] ---
_AD_NETWORK = 'everflow'
_AD_TYPE = 'spend'
_DATE_RANGE = 7

# è·å– Widget å‚æ•°
try:
    dbutils.widgets.text("ds", "", "Execution Date (YYYY-MM-DD)")
    ds_param = dbutils.widgets.get("ds")
except:
    ds_param = ""

if not ds_param:
    ds_param = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')

print(f"ğŸ“… Execution Date: {ds_param}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Task Logic

# COMMAND ----------

def fetch_spend_report_task(ds: str):
    """
    è·å– Everflow æ¶ˆè€—æŠ¥å‘Š
    
    ä½¿ç”¨ Everflow Reporting APIï¼Œæ”¯æŒå¤šä¸ª API Keyã€‚
    
    Args:
        ds: æ‰§è¡Œæ—¥æœŸ (YYYY-MM-DD)
    """
    end_dt = datetime.strptime(ds, '%Y-%m-%d')
    end_ds = ds
    start_dt = end_dt + timedelta(days=-(_DATE_RANGE))
    start_ds = start_dt.strftime('%Y-%m-%d')
    
    print(f"ğŸ“† Date Range: {start_ds} to {end_ds}")
    
    # è·å–é…ç½®
    cfg = helper.get_cfg(_AD_NETWORK)
    cfg_api_keys = cfg.get('api-key', '')
    api_keys = cfg_api_keys.split(',') if cfg_api_keys else []
    
    if not api_keys:
        print("âš ï¸ No API keys found in config.")
        return
    
    print(f"ğŸ“‹ Processing {len(api_keys)} API key(s)")
    
    # API è¯·æ±‚é…ç½®
    url = 'https://api.eflow.team/v1/advertisers/reporting/entity'
    json_data = {
        "from": start_ds,
        "to": end_ds,
        "timezone_id": 67,
        "currency_id": "USD",
        "columns": [
            {"column": "unix_timestamp"},
            {"column": "country_code"},
            {"column": "app_identifier"},
            {"column": "offer_id"},
            {"column": "advertiser_campaign_name"}
        ]
    }
    
    all_reports = []
    
    for key_index, api_key in enumerate(api_keys, start=1):
        print(f"\n--- Processing API Key {key_index}/{len(api_keys)} ---")
        
        headers = {
            'X-Eflow-API-Key': api_key,
            'content-type': 'application/json'
        }
        
        response = requests.post(url, headers=headers, json=json_data)
        
        if response.status_code != 200:
            logging.error(f"Failed to fetch report: {response.status_code}, {response.text}")
            continue
        
        # è§£æå“åº”æ•°æ®
        response_data = response.json()
        tables = response_data.get('table', [])
        
        for table in tables:
            # æå–ç»´åº¦ä¿¡æ¯
            dimensions = {}
            for column in table.get('columns', []):
                dimensions[column.get('column_type')] = column.get('label')
            
            # åˆå¹¶ç»´åº¦å’ŒæŠ¥å‘Šæ•°æ®
            reporting = table.get('reporting', {})
            item = dict(dimensions, **reporting)
            all_reports.append(item)
        
        print(f"   âœ… Fetched {len(tables)} table(s)")
    
    # ä¿å­˜æŠ¥å‘Š
    if all_reports:
        helper.save_report(
            ad_network=_AD_NETWORK,
            ad_type=_AD_TYPE,
            report=json.dumps(all_reports),
            exc_ds=ds,
            start_ds=start_ds,
            end_ds=end_ds
        )
        print(f"\nâœ… Saved {_AD_NETWORK} report with {len(all_reports)} record(s)")
    else:
        print(f"\nâš ï¸ No data to save for {_AD_NETWORK}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Execution

# COMMAND ----------

print(f"ğŸš€ Starting Job for {_AD_NETWORK}")

try:
    fetch_spend_report_task(ds_param)
    print("\nâœ… Job Finished Successfully")

except Exception as e:
    print(f"\nâŒ Job Failed: {e}")
    # on_failure_callback: å¤±è´¥æ—¶å‘é€é£ä¹¦é€šçŸ¥
    helper.failure_callback(str(e), f"{_AD_NETWORK}_spend_report")
    raise e

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Data Validation

# COMMAND ----------

env_mode = get_env_mode()
print(f"\nğŸ” Data Validation (ENV_MODE={env_mode})")

helper.validate_and_preview_data(_AD_TYPE, _AD_NETWORK)
