# Databricks notebook source
# MAGIC %md
# MAGIC # AppLovin Spend Report
# MAGIC
# MAGIC è¯¥ Notebook ä» AppLovin API è·å–å¹¿å‘Šæ¶ˆè€—æ•°æ®ã€‚
# MAGIC
# MAGIC - æ”¯æŒå¤šè´¦å·é…ç½®
# MAGIC - æŒ‰å¤©é€æ—¥æ‹‰å–æ•°æ®
# MAGIC - è‡ªåŠ¨è¿›è¡Œå­—æ®µåæ ‡å‡†åŒ–ï¼ˆå°å†™ã€ä¸‹åˆ’çº¿æ ¼å¼ï¼‰

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Setup & Imports

# COMMAND ----------

import re
import requests
from datetime import datetime, timedelta
import sys
import os
import pandas as pd

# åŠ¨æ€æ·»åŠ å½“å‰ç›®å½•åˆ° sys.path ä»¥åŠ è½½ utils
current_dir = os.getcwd()
if current_dir not in sys.path:
    sys.path.append(current_dir)

from utils import helper
from utils.config_manager import get_env_mode, setup_feishu_notify
import importlib
importlib.reload(helper)

# è®¾ç½® feishu-notify
Notifier = setup_feishu_notify()

print(f"ğŸ”§ Environment Mode: {get_env_mode()}")
print(f"âœ… Environment Setup Complete. Current Dir: {os.getcwd()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configuration

# COMMAND ----------

# --- [é…ç½®å‚æ•°] ---
_AD_NETWORK = 'applovin'
_AD_TYPE = 'spend'
_DATE_RANGE = 7

# è·å– Widget å‚æ•°
try:
    dbutils.widgets.text("ds", "", "Execution Date (YYYY-MM-DD)")
    ds_param = dbutils.widgets.get("ds")
except:
    ds_param = ""

if not ds_param:
    ds_param = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')

print(f"ğŸ“… Execution Date: {ds_param}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Core Functions

# COMMAND ----------

def _normalize_csv_header(csv_content: str) -> str:
    """
    æ ‡å‡†åŒ– CSV æ ‡é¢˜è¡Œï¼š
    - æ‰€æœ‰å­—æ®µåè½¬ä¸ºå°å†™
    - å¤šä¸ªå•è¯ç”¨ä¸‹åˆ’çº¿è¿æ¥ (å¦‚ "Conversion Rate" â†’ "conversion_rate")
    - ç‰¹æ®Šå­—æ®µåæ˜ å°„ï¼šday â†’ date, cost â†’ spend, campaign â†’ campaign_name, campaign_id_external â†’ campaign_id
    """
    lines = csv_content.strip().split('\n')
    if not lines:
        return csv_content
    
    # è·å–æ ‡é¢˜è¡Œ
    header = lines[0]
    columns = header.split(',')
    
    # å¤„ç†æ¯ä¸ªåˆ—å
    new_columns = []
    for col in columns:
        original_col = col.strip()
        
        # 1. å¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œç©ºæ ¼ï¼Œè½¬ä¸ºä¸‹åˆ’çº¿æ ¼å¼
        processed_col = original_col
        processed_col = processed_col.replace(' ', '_')     # ç©ºæ ¼è½¬ä¸‹åˆ’çº¿
        processed_col = processed_col.replace('-', '_')     # æ¨ªçº¿è½¬ä¸‹åˆ’çº¿
        processed_col = processed_col.replace('/', '_')     # æ–œæ è½¬ä¸‹åˆ’çº¿
        processed_col = processed_col.replace('(', '')      # ç§»é™¤å·¦æ‹¬å·
        processed_col = processed_col.replace(')', '')      # ç§»é™¤å³æ‹¬å·
        processed_col = processed_col.replace('%', 'rate')  # ç™¾åˆ†å·è½¬rate
        
        # 2. è½¬ä¸ºå°å†™
        processed_col = processed_col.lower()
        
        # 3. å¤„ç†è¿ç»­çš„ä¸‹åˆ’çº¿
        processed_col = re.sub(r'_+', '_', processed_col)  # å¤šä¸ªä¸‹åˆ’çº¿åˆå¹¶ä¸ºä¸€ä¸ª
        processed_col = processed_col.strip('_')           # å»é™¤é¦–å°¾ä¸‹åˆ’çº¿
        
        # 4. ç‰¹æ®Šå­—æ®µåæ˜ å°„
        field_mapping = {
            'day': 'date',
            'cost': 'spend',
            'campaign': 'campaign_name',
            'campaign_id_external': 'campaign_id'
        }
        if processed_col in field_mapping:
            processed_col = field_mapping[processed_col]
    
        new_columns.append(processed_col)
    
    # é‡æ–°ç»„åˆæ ‡é¢˜è¡Œ
    new_header = ','.join(new_columns)
    lines[0] = new_header
    
    return '\n'.join(lines)


def fetch_spend_report_task(ds: str):
    """
    è·å– AppLovin æ¶ˆè€—æŠ¥å‘Š
    
    Args:
        ds: æ‰§è¡Œæ—¥æœŸ (YYYY-MM-DD)
    
    ç‰¹ç‚¹ï¼š
    - æŒ‰å¤©é€æ—¥æ‹‰å–ï¼ˆé¿å…å¤§æ•°æ®é‡é—®é¢˜ï¼‰
    - æ”¯æŒå¤šè´¦å·é…ç½®
    - è‡ªåŠ¨æ ‡å‡†åŒ– CSV å­—æ®µå
    """
    print(f"ğŸ“Š Fetching {_AD_NETWORK} spend report for {ds}")
    
    end_dt = datetime.strptime(ds, '%Y-%m-%d')
    initial_start_dt = end_dt + timedelta(days=-_DATE_RANGE)
    
    cfg = helper.get_cfg(_AD_NETWORK)
    spend_accounts = cfg.get('spend', [])
    
    print(f"   ğŸ“± Found {len(spend_accounts)} account(s) to process")
    
    for item in spend_accounts:
        api_key = item.get('api_key')
        account_index = item.get('index')
        print(f"\n   ğŸ”‘ Processing account: {account_index}")
        
        start_dt = initial_start_dt
        while start_dt <= end_dt:
            start_ds = start_dt.strftime('%Y-%m-%d')
            end_ds = start_ds  # æŒ‰å¤©æ‹‰å–
            
            print(f"      ğŸ“† Fetching date: {start_ds}")
            
            req_opt = dict(
                url='https://r.applovin.com/report',
                params={
                    'api_key': api_key,
                    'start': start_ds,
                    'end': end_ds,
                    'columns': 'day,impressions,clicks,ctr,conversions,conversion_rate,average_cpa,average_cpc,country,campaign,traffic_source,ad_type,cost,sales,first_purchase,size,device_type,platform,campaign_package_name,campaign_id_external,campaign_ad_type,ad,ad_id,creative_set,creative_set_id,roas_0d,roas_1d,roas_3d,roas_7d,unique_purchasers_0d,unique_purchasers_1d,unique_purchasers_3d,unique_purchasers_7d,ret_1d,ret_3d,ret_7d',
                    'format': 'csv',
                    'report_type': 'advertiser',
                }
            )
            
            # å‘èµ·è¯·æ±‚
            resp = requests.get(**req_opt, timeout=(60, 300))
            
            if resp.status_code not in [200, 204, 422]:
                raise RuntimeError(
                    f'Failed to download {_AD_NETWORK} report for {end_ds}: {resp.status_code} {resp.text[:200]}'
                )
            
            if resp.text and resp.status_code == 200:
                resp.encoding = 'utf-8'
                report_str = resp.text
                
                # æ ‡å‡†åŒ– CSV å­—æ®µå
                report_str = _normalize_csv_header(report_str)
                
                # ä¿å­˜æŠ¥å‘Šï¼ˆä½¿ç”¨ custom å‚æ•°åŒºåˆ†è´¦å·ï¼‰
                helper.save_report(
                    ad_network=_AD_NETWORK,
                    ad_type=_AD_TYPE,
                    report=report_str,
                    exc_ds=ds,
                    start_ds=start_ds,
                    end_ds=end_ds,
                    custom=account_index
                )
                print(f"      âœ… Saved report for {start_ds}")
            else:
                print(f"      âš ï¸ No data for {start_ds}")
            
            start_dt += timedelta(days=1)
    
    print(f"\nâœ… Completed {_AD_NETWORK} spend report for {ds}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Execution

# COMMAND ----------

print(f"ğŸš€ Starting Job for {_AD_NETWORK}")

try:
    fetch_spend_report_task(ds_param)
    print("\nâœ… Job Finished Successfully")

except Exception as e:
    print(f"\nâŒ Job Failed: {e}")
    # on_failure_callback: å¤±è´¥æ—¶å‘é€é£ä¹¦é€šçŸ¥
    helper.failure_callback(str(e), f"{_AD_NETWORK}_{_AD_TYPE}_report")
    raise e

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Data Validation

# COMMAND ----------

env_mode = get_env_mode()
print(f"\nğŸ” Data Validation (ENV_MODE={env_mode})")

helper.validate_and_preview_data(_AD_TYPE, _AD_NETWORK)
