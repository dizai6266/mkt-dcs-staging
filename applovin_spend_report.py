# Databricks notebook source
# MAGIC %md
# MAGIC # AppLovin Spend Report
# MAGIC
# MAGIC ËØ• Notebook ‰ªé AppLovin API Ëé∑ÂèñÂπøÂëäÊ∂àËÄóÊï∞ÊçÆ„ÄÇ
# MAGIC
# MAGIC - ÊîØÊåÅÂ§öË¥¶Âè∑ÈÖçÁΩÆ
# MAGIC - ÊåâÂ§©ÈÄêÊó•ÊãâÂèñÊï∞ÊçÆ
# MAGIC - Ëá™Âä®ËøõË°åÂ≠óÊÆµÂêçÊ†áÂáÜÂåñÔºàÂ∞èÂÜô„ÄÅ‰∏ãÂàíÁ∫øÊ†ºÂºèÔºâ

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Setup & Imports

# COMMAND ----------

import re
import requests
from datetime import datetime, timedelta
import sys
import os
import pandas as pd

# Âä®ÊÄÅÊ∑ªÂä†ÂΩìÂâçÁõÆÂΩïÂà∞ sys.path ‰ª•Âä†ËΩΩ utils
current_dir = os.getcwd()
if current_dir not in sys.path:
    sys.path.append(current_dir)

from utils import helper
from utils.config_manager import get_env_mode, setup_feishu_notify
import importlib
importlib.reload(helper)

# ËÆæÁΩÆ feishu-notify
Notifier = setup_feishu_notify()

print(f"üîß Environment Mode: {get_env_mode()}")
print(f"‚úÖ Environment Setup Complete. Current Dir: {os.getcwd()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configuration

# COMMAND ----------

# --- [ÈÖçÁΩÆÂèÇÊï∞] ---
_AD_NETWORK = 'applovin'
_AD_TYPE = 'spend'
_DATE_RANGE = 7

# Ëé∑Âèñ Widget ÂèÇÊï∞
try:
    dbutils.widgets.text("ds", "", "Execution Date (YYYY-MM-DD)")
    ds_param = dbutils.widgets.get("ds")
except:
    ds_param = ""

if not ds_param:
    ds_param = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')

print(f"üìÖ Execution Date: {ds_param}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Core Functions

# COMMAND ----------

def _normalize_csv_header(csv_content: str) -> str:
    """
    Ê†áÂáÜÂåñ CSV Ê†áÈ¢òË°åÔºö
    - ÊâÄÊúâÂ≠óÊÆµÂêçËΩ¨‰∏∫Â∞èÂÜô
    - Â§ö‰∏™ÂçïËØçÁî®‰∏ãÂàíÁ∫øËøûÊé• (Â¶Ç "Conversion Rate" ‚Üí "conversion_rate")
    - ÁâπÊÆäÂ≠óÊÆµÂêçÊò†Â∞ÑÔºöday ‚Üí date, cost ‚Üí spend, campaign ‚Üí campaign_name, campaign_id_external ‚Üí campaign_id
    """
    lines = csv_content.strip().split('\n')
    if not lines:
        return csv_content
    
    # Ëé∑ÂèñÊ†áÈ¢òË°å
    header = lines[0]
    columns = header.split(',')
    
    # Â§ÑÁêÜÊØè‰∏™ÂàóÂêç
    new_columns = []
    for col in columns:
        original_col = col.strip()
        
        # 1. Â§ÑÁêÜÁâπÊÆäÂ≠óÁ¨¶ÂíåÁ©∫Ê†ºÔºåËΩ¨‰∏∫‰∏ãÂàíÁ∫øÊ†ºÂºè
        processed_col = original_col
        processed_col = processed_col.replace(' ', '_')     # Á©∫Ê†ºËΩ¨‰∏ãÂàíÁ∫ø
        processed_col = processed_col.replace('-', '_')     # Ê®™Á∫øËΩ¨‰∏ãÂàíÁ∫ø
        processed_col = processed_col.replace('/', '_')     # ÊñúÊù†ËΩ¨‰∏ãÂàíÁ∫ø
        processed_col = processed_col.replace('(', '')      # ÁßªÈô§Â∑¶Êã¨Âè∑
        processed_col = processed_col.replace(')', '')      # ÁßªÈô§Âè≥Êã¨Âè∑
        processed_col = processed_col.replace('%', 'rate')  # ÁôæÂàÜÂè∑ËΩ¨rate
        
        # 2. ËΩ¨‰∏∫Â∞èÂÜô
        processed_col = processed_col.lower()
        
        # 3. Â§ÑÁêÜËøûÁª≠ÁöÑ‰∏ãÂàíÁ∫ø
        processed_col = re.sub(r'_+', '_', processed_col)  # Â§ö‰∏™‰∏ãÂàíÁ∫øÂêàÂπ∂‰∏∫‰∏Ä‰∏™
        processed_col = processed_col.strip('_')           # ÂéªÈô§È¶ñÂ∞æ‰∏ãÂàíÁ∫ø
        
        # 4. ÁâπÊÆäÂ≠óÊÆµÂêçÊò†Â∞Ñ
        field_mapping = {
            'day': 'date',
            'cost': 'spend',
            'campaign': 'campaign_name',
            'campaign_id_external': 'campaign_id'
        }
        if processed_col in field_mapping:
            processed_col = field_mapping[processed_col]
    
        new_columns.append(processed_col)
    
    # ÈáçÊñ∞ÁªÑÂêàÊ†áÈ¢òË°å
    new_header = ','.join(new_columns)
    lines[0] = new_header
    
    return '\n'.join(lines)


def fetch_spend_report_task(ds: str):
    """
    Ëé∑Âèñ AppLovin Ê∂àËÄóÊä•Âëä
    
    Args:
        ds: ÊâßË°åÊó•Êúü (YYYY-MM-DD)
    
    ÁâπÁÇπÔºö
    - ÊåâÂ§©ÈÄêÊó•ÊãâÂèñÔºàÈÅøÂÖçÂ§ßÊï∞ÊçÆÈáèÈóÆÈ¢òÔºâ
    - ÊîØÊåÅÂ§öË¥¶Âè∑ÈÖçÁΩÆ
    - Ëá™Âä®Ê†áÂáÜÂåñ CSV Â≠óÊÆµÂêç
    """
    print(f"üìä Fetching {_AD_NETWORK} spend report for {ds}")
    
    end_dt = datetime.strptime(ds, '%Y-%m-%d')
    initial_start_dt = end_dt + timedelta(days=-_DATE_RANGE)
    
    cfg = helper.get_cfg(_AD_NETWORK)
    spend_accounts = cfg.get('spend', [])
    
    print(f"   üì± Found {len(spend_accounts)} account(s) to process")
    
    for item in spend_accounts:
        api_key = item.get('api_key')
        account_index = item.get('index')
        
        # ‰ºòÂÖà‰ΩøÁî®ÈÖçÁΩÆ‰∏≠ÁöÑ account_id Êàñ account_nameÔºåÂ¶ÇÊûúÊ≤°ÊúâÂàô‰ΩøÁî® account_id Êò†Â∞Ñ
        account_identifier = item.get('account_id') or item.get('account_name')
        
        # Â¶ÇÊûúÊ≤°ÊúâÈÖçÁΩÆ account_id Êàñ account_nameÔºåÂ∞ùËØï‰ΩøÁî®Êò†Â∞ÑÔºàÂêëÂêéÂÖºÂÆπÔºâ
        if not account_identifier:
            # API Key Êò†Â∞ÑÔºöÊ†πÊçÆ spend ÈÖçÁΩÆÔºå‰ΩøÁî® api_key ÂâçÂá†‰Ωç‰Ωú‰∏∫Ê†áËØÜ
            # index 1 -> api_key "uTAga", index 2 -> api_key "ND6W", index 3 -> api_key "VA3d"
            API_KEY_MAP = {
                1: 'uTAga',
                2: 'ND6W',
                3: 'VA3d'
            }
            account_identifier = API_KEY_MAP.get(account_index)
        
        if not account_identifier:
            print(f"‚ö†Ô∏è Skipping account with index {account_index} (no account_id or account_name found)")
            continue
        
        print(f"\n   üîë Processing account: {account_identifier} (index: {account_index})")
        
        start_dt = initial_start_dt
        while start_dt <= end_dt:
            start_ds = start_dt.strftime('%Y-%m-%d')
            end_ds = start_ds  # ÊåâÂ§©ÊãâÂèñ
            
            print(f"      üìÜ Fetching date: {start_ds}")
            
            req_opt = dict(
                url='https://r.applovin.com/report',
                params={
                    'api_key': api_key,
                    'start': start_ds,
                    'end': end_ds,
                    'columns': 'day,impressions,clicks,ctr,conversions,conversion_rate,average_cpa,average_cpc,country,campaign,traffic_source,ad_type,cost,sales,first_purchase,size,device_type,platform,campaign_package_name,campaign_id_external,campaign_ad_type,ad,ad_id,creative_set,creative_set_id,roas_0d,roas_1d,roas_3d,roas_7d,unique_purchasers_0d,unique_purchasers_1d,unique_purchasers_3d,unique_purchasers_7d,ret_1d,ret_3d,ret_7d',
                    'format': 'csv',
                    'report_type': 'advertiser',
                }
            )
            
            # ÂèëËµ∑ËØ∑Ê±Ç
            resp = requests.get(**req_opt, timeout=(60, 300))
            
            if resp.status_code not in [200, 204, 422]:
                raise RuntimeError(
                    f'Failed to download {_AD_NETWORK} report for {end_ds}: {resp.status_code} {resp.text[:200]}'
                )
            
            if resp.text and resp.status_code == 200:
                resp.encoding = 'utf-8'
                report_str = resp.text
                
                # Ê†áÂáÜÂåñ CSV Â≠óÊÆµÂêç
                report_str = _normalize_csv_header(report_str)
                
                # ‰øùÂ≠òÊä•ÂëäÔºà‰ΩøÁî®ÁúüÂÆûË¥¶Âè∑Ê†áËØÜÂå∫ÂàÜË¥¶Âè∑Ôºâ
                helper.save_report(
                    ad_network=_AD_NETWORK,
                    ad_type=_AD_TYPE,
                    report=report_str,
                    exc_ds=ds,
                    start_ds=start_ds,
                    end_ds=end_ds,
                    custom=account_identifier
                )
                print(f"      ‚úÖ Saved report for {start_ds}")
            else:
                print(f"      ‚ö†Ô∏è No data for {start_ds}")
            
            start_dt += timedelta(days=1)
    
    print(f"\n‚úÖ Completed {_AD_NETWORK} spend report for {ds}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Execution

# COMMAND ----------

print(f"üöÄ Starting Job for {_AD_NETWORK}")

try:
    fetch_spend_report_task(ds_param)
    print("\n‚úÖ Job Finished Successfully")

except Exception as e:
    print(f"\n‚ùå Job Failed: {e}")
    # on_failure_callback: Â§±Ë¥•Êó∂ÂèëÈÄÅÈ£û‰π¶ÈÄöÁü•
    helper.failure_callback(str(e), f"{_AD_NETWORK}_{_AD_TYPE}_report")
    raise e

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Data Validation

# COMMAND ----------

env_mode = get_env_mode()
print(f"\nüîç Data Validation (ENV_MODE={env_mode})")

helper.validate_and_preview_data(_AD_TYPE, _AD_NETWORK)
