# Databricks notebook source
# MAGIC %md
# MAGIC # Ayet Studios Spend Report
# MAGIC
# MAGIC è¯¥ Notebook ä» AyetStudios API è·å–å¹¿å‘Šæ¶ˆè€—æ•°æ®ã€‚
# MAGIC
# MAGIC - æ”¯æŒå¤šæ—¥å›æº¯
# MAGIC - æŒ‰å›½å®¶ç»´åº¦æ‹†åˆ†æ•°æ®
# MAGIC - è‡ªåŠ¨å¤„ç† API å“åº”æ ¼å¼

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Setup & Imports

# COMMAND ----------

import json
import logging
import requests
from datetime import datetime, timedelta
from time import sleep
import sys
import os
import pandas as pd

# åŠ¨æ€æ·»åŠ å½“å‰ç›®å½•åˆ° sys.path ä»¥åŠ è½½ utils
current_dir = os.getcwd()
if current_dir not in sys.path:
    sys.path.append(current_dir)

from utils import helper
from utils.config_manager import get_env_mode, setup_feishu_notify
import importlib
importlib.reload(helper)

# è®¾ç½® feishu-notify
Notifier = setup_feishu_notify()

print(f"ğŸ”§ Environment Mode: {get_env_mode()}")
print(f"âœ… Environment Setup Complete. Current Dir: {os.getcwd()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Configuration

# COMMAND ----------

# --- [é…ç½®å‚æ•°] ---
_AD_NETWORK = 'ayet'
_AD_TYPE = 'spend'
_DATE_RANGE = 7

# è·å– Widget å‚æ•°
try:
    dbutils.widgets.text("ds", "", "Execution Date (YYYY-MM-DD)")
    ds_param = dbutils.widgets.get("ds")
except:
    ds_param = ""

if not ds_param:
    ds_param = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')

print(f"ğŸ“… Execution Date: {ds_param}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Core Functions

# COMMAND ----------

def _fetch_detailed_data(api_key: str, target_date: str) -> list:
    """
    ä» AyetStudios API è·å–æŒ‡å®šæ—¥æœŸçš„è¯¦ç»†æ•°æ®
    
    Args:
        api_key: API å¯†é’¥
        target_date: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)
    
    Returns:
        list: å¤„ç†åçš„è®°å½•åˆ—è¡¨
    """
    url = "https://ayetstudios.com/api2/reporting/countries"
    
    params = {
        'apiKey': api_key,
        'startDate': target_date,
        'endDate': target_date
    }
    
    print(f"      ğŸ“¡ Fetching data for {target_date}...")
    
    try:
        response = requests.get(url, params=params, timeout=30)
        
        if response.status_code != 200:
            logging.error(f"Request failed, status code: {response.status_code}, response: {response.text}")
            return []
        
        data = response.json()
        if data.get('status') != 'success':
            logging.error(f"API returned non-success status: {data.get('status')}")
            return []
        
        return _process_detailed_data(data, target_date)
        
    except Exception as e:
        logging.error(f"Request exception: {e}")
        return []


def _process_detailed_data(data: dict, query_date: str) -> list:
    """
    å¤„ç† API å“åº”ï¼Œæå–å¹¶å±•å¼€è¯¦ç»†çš„ campaign å’Œ country æ•°æ®
    
    Args:
        data: API å“åº”æ•°æ®
        query_date: æŸ¥è¯¢æ—¥æœŸ
    
    Returns:
        list: å¤„ç†åçš„è®°å½•åˆ—è¡¨
    """
    campaigns = data.get('campaignData', [])
    detailed = data.get('detailed', {})
    
    if not detailed:
        logging.warning("No detailed data found")
        return []
    
    # åˆ›å»º campaign æ˜ å°„
    campaign_map = {str(c.get('campaignId')): c for c in campaigns}
    
    # å¤„ç†è¯¦ç»†æ•°æ®
    detailed_results = []
    
    for campaign_id, countries in detailed.items():
        campaign_info = campaign_map.get(campaign_id, {})
        identifier = campaign_info.get('identifier', 'Unknown')
        package_name = campaign_info.get('packageName', 'Unknown')
        platform = campaign_info.get('platform', 'Unknown')
        
        for country, metrics in countries.items():
            record = {
                'date': query_date,
                'campaign_id': campaign_id,
                'identifier': identifier,
                'package_name': package_name,
                'platform': platform,
                'country_code': country,
                'impressions': metrics.get('impressions', 0),
                'clicks': metrics.get('clicks', 0),
                'conversions': metrics.get('conversions', 0),
                'adspend': metrics.get('adspend', 0),
                'conversion_rate': metrics.get('conversion_rate', 0)
            }
            detailed_results.append(record)
    
    return detailed_results


def fetch_spend_report_task(ds: str):
    """
    è·å– AyetStudios æ¶ˆè€—æŠ¥å‘Š
    
    Args:
        ds: æ‰§è¡Œæ—¥æœŸ (YYYY-MM-DD)
    
    ç‰¹ç‚¹ï¼š
    - æŒ‰å¤©é€æ—¥æ‹‰å–
    - æŒ‰ campaign å’Œ country ç»´åº¦å±•å¼€
    - åˆå¹¶å¤šæ—¥æ•°æ®åç»Ÿä¸€ä¿å­˜
    """
    print(f"ğŸ“Š Fetching {_AD_NETWORK} spend report for {ds}")
    
    cfg = helper.get_cfg(_AD_NETWORK)
    api_key = cfg.get('apiKey')
    
    if not api_key:
        raise ValueError("API key not found in configuration")
    
    all_reports = []
    failed_dates = []
    
    print(f"   ğŸ“† Fetching {_DATE_RANGE} days of data ending on {ds}")
    
    # å¾ªç¯æ‹‰å–è¿‡å» N å¤©çš„æ•°æ®
    for i in range(_DATE_RANGE):
        date_obj = datetime.strptime(ds, '%Y-%m-%d') - timedelta(days=i)
        query_date = date_obj.strftime('%Y-%m-%d')
        
        try:
            detailed_data = _fetch_detailed_data(api_key, query_date)
            
            if detailed_data:
                all_reports.extend(detailed_data)
                print(f"      âœ… {query_date}: {len(detailed_data)} records")
            else:
                print(f"      âš ï¸ {query_date}: No data")
                failed_dates.append(query_date)
            
        except Exception as e:
            logging.error(f"Failed to fetch data for {query_date}: {e}")
            failed_dates.append(query_date)
        
        # é¿å… API é™æµ
        if i < _DATE_RANGE - 1:
            sleep(1)
    
    # è®°å½•å¤±è´¥æƒ…å†µ
    if failed_dates:
        logging.warning(f"Failed to fetch data for dates: {failed_dates}")
    
    if not all_reports:
        logging.error(f"No data retrieved for any date in the range")
        return
    
    # æŒ‰æ—¥æœŸæ’åº
    all_reports.sort(key=lambda x: x['date'])
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    start_date = (datetime.strptime(ds, '%Y-%m-%d') - timedelta(days=_DATE_RANGE - 1)).strftime('%Y-%m-%d')
    end_date = ds
    
    print(f"\n   ğŸ“Š Total records: {len(all_reports)} ({start_date} to {end_date})")
    
    # æŒ‰æ—¥æœŸç»Ÿè®¡
    date_counts = {}
    for record in all_reports:
        date = record['date']
        date_counts[date] = date_counts.get(date, 0) + 1
    print(f"   ğŸ“‹ Records per date: {date_counts}")
    
    # ä¿å­˜æŠ¥å‘Š
    helper.save_report(
        ad_network=_AD_NETWORK,
        ad_type=_AD_TYPE,
        report=json.dumps(all_reports),
        exc_ds=ds,
        start_ds=start_date,
        end_ds=end_date
    )
    
    print(f"\nâœ… Saved {_AD_NETWORK} report for {start_date} to {end_date}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Execution

# COMMAND ----------

print(f"ğŸš€ Starting Job for {_AD_NETWORK}")

try:
    fetch_spend_report_task(ds_param)
    print("\nâœ… Job Finished Successfully")

except Exception as e:
    print(f"\nâŒ Job Failed: {e}")
    # on_failure_callback: å¤±è´¥æ—¶å‘é€é£ä¹¦é€šçŸ¥
    helper.failure_callback(str(e), f"{_AD_NETWORK}_{_AD_TYPE}_report")
    raise e

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Data Validation

# COMMAND ----------

env_mode = get_env_mode()
print(f"\nğŸ” Data Validation (ENV_MODE={env_mode})")

helper.validate_and_preview_data(_AD_TYPE, _AD_NETWORK)
